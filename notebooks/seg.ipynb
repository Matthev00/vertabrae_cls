{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ca189249a8490e960256a039f6c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Model pobrany do: /home/mateusz/PW/INZ/vertabrae_cls/models/monai_seg\n"
     ]
    }
   ],
   "source": [
    "model_dir = snapshot_download(repo_id=\"MONAI/wholeBody_ct_segmentation\", local_dir=\"../models/monai_seg\")\n",
    "print(\"📥 Model pobrany do:\", model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegResNet(\n",
       "  (act_mod): ReLU(inplace=True)\n",
       "  (convInit): Convolution(\n",
       "    (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "  )\n",
       "  (down_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (norm1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (norm1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (norm1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (norm1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (norm1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (norm1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (norm1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (norm1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (norm2): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "        (act): ReLU(inplace=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_samples): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='trilinear')\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='trilinear')\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='trilinear')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_final): Sequential(\n",
       "    (0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Convolution(\n",
       "      (conv): Conv3d(32, 105, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout3d(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from monai.bundle import ConfigParser\n",
    "\n",
    "# Ścieżka do katalogu z pobranym modelem MONAI Bundle\n",
    "model_dir = \"../models/monai_seg\"\n",
    "model_file = \"models/model.pt\"  # zakładamy, że w folderze models jest plik model.pt\n",
    "\n",
    "# Wczytanie konfiguracji inferencji z bundle (określa m.in. architekturę sieci i transformacje)\n",
    "config_path = os.path.join(model_dir, \"configs\", \"inference.json\")\n",
    "parser = ConfigParser()\n",
    "parser.read_config(config_path)\n",
    "\n",
    "# Utworzenie modelu zgodnie z konfiguracją (znalezienie definicji sieci w configu)\n",
    "model = parser.get_parsed_content(\"network_def\")\n",
    "# Załadowanie wag modelu\n",
    "weights = torch.load(os.path.join(model_dir, model_file), map_location=\"cpu\")\n",
    "# Jeśli wagi są zapisane pod kluczem \"state_dict\", wyciągnij go:\n",
    "if isinstance(weights, dict) and \"state_dict\" in weights:\n",
    "    weights = weights[\"state_dict\"]\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar modelu: 71.71402359008789 MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.numel() * param.element_size()\n",
    "print(\"Rozmiar modelu:\", param_size / 1024 / 1024, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Błąd wczytywania DICOM z folderu /media/mateusz/DATA/downloads/inz/RM M 40: Nie znaleziono serii DICOM w folderze: /media/mateusz/DATA/downloads/inz/RM M 40\n",
      "Wczytano tensor o kształcie: torch.Size([2, 1228, 512]) dla serii:  z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S1000\n",
      "Metadane: {'spacing': (0.9765625, 0.9765625, 353.5533905932738), 'direction': (0.0, 0.0, -1.0, 1.0, 0.0, 0.0, 0.0, -1.0, 0.0), 'origin': (0.0, -160.0, 1199.31875)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: In /tmp/SimpleITK-build/ITK/Modules/IO/GDCM/src/itkGDCMSeriesFileNames.cxx, line 114\n",
      "GDCMSeriesFileNames (0x624aca182650): No Series can be found, make sure your restrictions are not too strong\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytano tensor o kształcie: torch.Size([249, 512, 512]) dla serii: GLOWA PRZEGLAD, iDose (4)  z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S2010\n",
      "Metadane: {'spacing': (0.486328125, 0.486328125, 0.7499999999999996), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-109.56, -61.392, 974.61)}\n",
      "Wczytano tensor o kształcie: torch.Size([374, 512, 512]) dla serii: GLOWA KOSCI, iDose (4) z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S2020\n",
      "Metadane: {'spacing': (0.486328125, 0.486328125, 0.4999999999999997), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-109.56, -61.392, 974.36)}\n",
      "Wczytano tensor o kształcie: torch.Size([411, 512, 512]) dla serii: SZYJNY KOSCI, iDose (4)  z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S2030\n",
      "Metadane: {'spacing': (0.474609375, 0.474609375, 0.5), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-106.434, -71.595, 848.76)}\n",
      "Wczytano tensor o kształcie: torch.Size([273, 512, 512]) dla serii: SZYJNY PRZEGLAD  z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S2040\n",
      "Metadane: {'spacing': (0.474609375, 0.474609375, 0.75), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-104.49, -71.595, 849.01)}\n",
      "Wczytano tensor o kształcie: torch.Size([237, 512, 512]) dla serii: TWAR PRZEL z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S2050\n",
      "Metadane: {'spacing': (0.431640625, 0.431640625, 0.7500000000000004), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-113.594, -105.585, 966.42)}\n",
      "Wczytano tensor o kształcie: torch.Size([356, 512, 512]) dla serii: TWARZOCZASZKA KO, iDose (4)  z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S2060\n",
      "Metadane: {'spacing': (0.431640625, 0.431640625, 0.5000000000000003), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-113.594, -105.585, 966.17)}\n",
      "Wczytano tensor o kształcie: torch.Size([97, 512, 512]) dla serii: TWARZOCZASZKA KO, iDose (4)  z folderu: /media/mateusz/DATA/downloads/inz/RM M 40/S3020\n",
      "Metadane: {'spacing': (0.544921875, 0.544921875, 0.5), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-142.569, -134.595, 966.17)}\n",
      "Wybrano tensor dla serii: SZYJNY KOSCI, iDose (4)  (priorytet: kosci)\n",
      "Kształt wybranego tensora: torch.Size([411, 512, 512])\n",
      "Metadane wybranego tensora: {'spacing': (0.474609375, 0.474609375, 0.5), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0), 'origin': (-106.434, -71.595, 848.76)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import torch  # lub import tensorflow as tf\n",
    "\n",
    "def load_dicom_series_with_metadata(dicom_dir):\n",
    "    \"\"\"\n",
    "    Wczytuje serię DICOM z folderu, zwraca tensor, opis serii oraz metadane.\n",
    "    \"\"\"\n",
    "    # Znajdź wszystkie pliki DICOM w folderze\n",
    "    reader = sitk.ImageSeriesReader()\n",
    "    dicom_series = reader.GetGDCMSeriesFileNames(dicom_dir)\n",
    "    if not dicom_series:\n",
    "        raise ValueError(f\"Nie znaleziono serii DICOM w folderze: {dicom_dir}\")\n",
    "    reader.SetFileNames(dicom_series)\n",
    "\n",
    "    # Wczytaj serię DICOM jako obraz 3D\n",
    "    image = reader.Execute()\n",
    "\n",
    "    # Przekształć na NumPy array\n",
    "    image_array = sitk.GetArrayFromImage(image)  # Shape: (z, y, x)\n",
    "\n",
    "    # Konwersja na tensor (PyTorch)\n",
    "    tensor = torch.tensor(image_array, dtype=torch.float32)  # Shape: (z, y, x)\n",
    "\n",
    "    # Pobierz metadane z obrazu\n",
    "    spacing = image.GetSpacing()  # Rozdzielczość voxelowa (x, y, z)\n",
    "    direction = image.GetDirection()  # Orientacja\n",
    "    origin = image.GetOrigin()  # Początek układu współrzędnych\n",
    "\n",
    "    # Pobierz opis serii z pierwszego pliku\n",
    "    first_file = dicom_series[0]\n",
    "    file_reader = sitk.ImageFileReader()\n",
    "    file_reader.SetFileName(first_file)\n",
    "    file_reader.ReadImageInformation()\n",
    "    series_description = file_reader.GetMetaData(\"0008|103e\") if file_reader.HasMetaDataKey(\"0008|103e\") else \"Unknown\"\n",
    "\n",
    "    # Zwróć tensor, opis serii i metadane\n",
    "    metadata = {\n",
    "        \"spacing\": spacing,\n",
    "        \"direction\": direction,\n",
    "        \"origin\": origin\n",
    "    }\n",
    "    return tensor, series_description, metadata\n",
    "\n",
    "# Przykład użycia\n",
    "dicom_root_folder = \"/media/mateusz/DATA/downloads/inz/RM M 40\"\n",
    "series_tensors = {}\n",
    "\n",
    "for root, dirs, files in os.walk(dicom_root_folder):\n",
    "    if files:  # Jeśli folder zawiera pliki\n",
    "        try:\n",
    "            tensor, series_description, metadata = load_dicom_series_with_metadata(root)\n",
    "            series_tensors[series_description] = {\"tensor\": tensor, \"metadata\": metadata}\n",
    "            print(f\"Wczytano tensor o kształcie: {tensor.shape} dla serii: {series_description} z folderu: {root}\")\n",
    "            print(f\"Metadane: {metadata}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd wczytywania DICOM z folderu {root}: {e}\")\n",
    "\n",
    "# Wybierz tensor zgodnie z priorytetami\n",
    "selected_tensor = None\n",
    "selected_description = None\n",
    "selected_metadata = None\n",
    "\n",
    "# 1. Szukaj serii \"kosci\"\n",
    "for series_description, data in series_tensors.items():\n",
    "    if \"szyjny kosci\" in series_description.lower():\n",
    "        selected_tensor = data[\"tensor\"]\n",
    "        selected_metadata = data[\"metadata\"]\n",
    "        selected_description = series_description\n",
    "        print(f\"Wybrano tensor dla serii: {series_description} (priorytet: kosci)\")\n",
    "        break\n",
    "\n",
    "# 2. Jeśli nie znaleziono \"kosci\", szukaj \"miekkie\"\n",
    "if selected_tensor is None:\n",
    "    for series_description, data in series_tensors.items():\n",
    "        if \"miekkie\" in series_description.lower():\n",
    "            selected_tensor = data[\"tensor\"]\n",
    "            selected_metadata = data[\"metadata\"]\n",
    "            selected_description = series_description\n",
    "            print(f\"Wybrano tensor dla serii: {series_description} (priorytet: miekkie)\")\n",
    "            break\n",
    "\n",
    "# 3. Jeśli nie znaleziono \"kosci\" ani \"miekkie\", wybierz największy tensor\n",
    "if selected_tensor is None:\n",
    "    max_size = 0\n",
    "    for series_description, data in series_tensors.items():\n",
    "        tensor_size = data[\"tensor\"].numel()  # Liczba elementów w tensorze\n",
    "        if tensor_size > max_size:\n",
    "            max_size = tensor_size\n",
    "            selected_tensor = data[\"tensor\"]\n",
    "            selected_metadata = data[\"metadata\"]\n",
    "            selected_description = series_description\n",
    "    if selected_tensor is not None:\n",
    "        print(f\"Wybrano największy tensor dla serii: {selected_description} (rozmiar: {max_size})\")\n",
    "\n",
    "# Sprawdź, czy wybrano tensor\n",
    "if selected_tensor is None:\n",
    "    print(\"Nie znaleziono odpowiedniego tensora.\")\n",
    "else:\n",
    "    print(f\"Kształt wybranego tensora: {selected_tensor.shape}\")\n",
    "    print(f\"Metadane wybranego tensora: {selected_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([411, 512, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import ScaleIntensityRanged, Compose, LoadImaged, EnsureChannelFirstd, Spacingd, Orientationd, NormalizeIntensityd, EnsureTyped, ScaleIntensityd\n",
    "\n",
    "# Definicja transformacji\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\"]),  # Ładowanie obrazu\n",
    "    EnsureTyped(keys=[\"image\"]),  # Konwersja na typ zgodny z PyTorch/NumPy\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),  # Upewnienie się, że kanały są na pierwszej osi\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),  # Ustawienie orientacji na RAS\n",
    "    Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 1.5), mode=\"bilinear\"),  # Resampling do zadanych wymiarów voxelowych\n",
    "    NormalizeIntensityd(keys=[\"image\"], nonzero=True),  # Normalizacja intensywności na podstawie wartości niezerowych\n",
    "    ScaleIntensityd(keys=[\"image\"], minv=-1.0, maxv=1.0)  # Skalowanie intensywności do zakresu [-1.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensor_as_nrrd(tensor, file_path, metadata):\n",
    "    \"\"\"\n",
    "    Zapisuje tensor jako plik NRRD z metadanymi.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor do zapisania.\n",
    "        file_path (str): Ścieżka do pliku wyjściowego (powinna kończyć się na .nrrd).\n",
    "        metadata (dict): Metadane obrazu, zawierające klucze: 'spacing', 'direction', 'origin'.\n",
    "    \"\"\"\n",
    "    # Konwersja tensora na NumPy array\n",
    "    array = tensor.numpy()  # Shape: (z, y, x)\n",
    "    \n",
    "    # Konwersja NumPy array na SimpleITK Image\n",
    "    image = sitk.GetImageFromArray(array)  # SimpleITK interpretuje dane jako (z, y, x)\n",
    "    \n",
    "    # Ustaw metadane\n",
    "    if \"spacing\" in metadata:\n",
    "        image.SetSpacing(metadata[\"spacing\"])  # Rozdzielczość voxelowa (x, y, z)\n",
    "    if \"direction\" in metadata:\n",
    "        image.SetDirection(metadata[\"direction\"])  # Orientacja\n",
    "    if \"origin\" in metadata:\n",
    "        image.SetOrigin(metadata[\"origin\"])  # Początek układu współrzędnych\n",
    "    \n",
    "    # Zapis do pliku NRRD\n",
    "    sitk.WriteImage(image, file_path)\n",
    "    print(f\"Tensor zapisany jako NRRD z metadanymi: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor zapisany jako NRRD z metadanymi: /media/mateusz/DATA/tmp/tmph04vw1g_/output.nrrd\n",
      "Tensor zapisany jako plik tymczasowy: /media/mateusz/DATA/tmp/tmph04vw1g_/output.nrrd\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_file = os.path.join(temp_dir, \"output.nrrd\")\n",
    "    \n",
    "    # Zapisz tensor jako plik NIfTI\n",
    "    save_tensor_as_nrrd(selected_tensor, temp_file, selected_metadata)\n",
    "    print(f\"Tensor zapisany jako plik tymczasowy: {temp_file}\")\n",
    "    \n",
    "    # Przygotuj dane wejściowe dla transformacji\n",
    "    data = {\"image\": temp_file}\n",
    "    \n",
    "    # Zastosuj transformacje\n",
    "\n",
    "    transformed_data = transforms(data)\n",
    "    transformed_tensor = transformed_data[\"image\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 163, 163, 138])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = transformed_tensor.unsqueeze(0).to(\"cpu\")  # Dodaj wymiar wsadu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "model.eval().to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    output = sliding_window_inference(\n",
    "        inputs=input_tensor,\n",
    "        roi_size=(96, 96, 96),  \n",
    "        sw_batch_size=1,\n",
    "        predictor=model,\n",
    "        overlap=0.5,\n",
    "        mode=\"gaussian\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label  0: 3383701 voxeli\n",
      "Label 50: 85584 voxeli\n",
      "Label 15: 26625 voxeli\n",
      "Label 13: 25004 voxeli\n",
      "Label 85: 17594 voxeli\n",
      "Label 101: 17285 voxeli\n",
      "Label 100: 15126 voxeli\n",
      "Label 87: 9231 voxeli\n",
      "Label 33: 8267 voxeli\n",
      "Label 34: 8199 voxeli\n",
      "Label 32: 6330 voxeli\n",
      "Label 35: 5832 voxeli\n",
      "Label 40: 5786 voxeli\n",
      "Label 43: 5176 voxeli\n",
      "Label 86: 4897 voxeli\n",
      "Label 36: 4609 voxeli\n",
      "Label 84: 4388 voxeli\n",
      "Label 37: 4370 voxeli\n",
      "Label 38: 3820 voxeli\n",
      "Label 41: 3613 voxeli\n",
      "Label 39: 3297 voxeli\n",
      "Label 42: 3059 voxeli\n",
      "Label 71: 2578 voxeli\n",
      "Label 59: 2493 voxeli\n",
      "Label 70: 2108 voxeli\n",
      "Label 58: 2011 voxeli\n",
      "Label 72: 1933 voxeli\n",
      "Label 60: 1757 voxeli\n",
      "Label 73: 898 voxeli\n",
      "Label 31: 530 voxeli\n",
      "Label 61: 276 voxeli\n",
      "Label 93: 100 voxeli\n",
      "Label 83: 32 voxeli\n",
      "Label 74: 8 voxeli\n",
      "Label 28: 4 voxeli\n",
      "Label 21: 1 voxeli\n"
     ]
    }
   ],
   "source": [
    "segmentation = output.argmax(dim=1).squeeze().cpu().numpy()  # shape: (D, H, W)\n",
    "\n",
    "# Policz unikalne etykiety i ile ich jest\n",
    "labels, counts = np.unique(segmentation, return_counts=True)\n",
    "\n",
    "# Pokaż w formie słownika\n",
    "label_counts = dict(zip(labels, counts))\n",
    "\n",
    "# Posortowane i wypisane\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"Label {label:>2}: {count} voxeli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 105, 163, 163, 138])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Załóżmy że output ma kształt (1, C, D, H, W)\n",
    "output_np = output.argmax(dim=1).squeeze().cpu().numpy()  # (D, H, W)\n",
    "\n",
    "z = output_np.shape[2] // 2  # środkowy slice\n",
    "\n",
    "plt.imshow(output_np[:, :, z], cmap=\"nipy_spectral\")\n",
    "plt.title(f\"Segmentacja - slice {z}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEBRAE_MAP = {\n",
    "    \"L5\": 18,\n",
    "    \"L4\": 19,\n",
    "    \"L3\": 20,\n",
    "    \"L2\": 21,\n",
    "    \"L1\": 22,\n",
    "    \"Th12\": 23,\n",
    "    \"Th11\": 24,\n",
    "    \"Th10\": 25,\n",
    "    \"Th9\": 26,\n",
    "    \"Th8\": 27,\n",
    "    \"Th7\": 28,\n",
    "    \"Th6\": 29,\n",
    "    \"Th5\": 30,\n",
    "    \"Th4\": 31,\n",
    "    \"Th3\": 32,\n",
    "    \"Th2\": 33,\n",
    "    \"Th1\": 34,\n",
    "    \"C7\": 35,\n",
    "    \"C6\": 36,\n",
    "    \"C5\": 37,\n",
    "    \"C4\": 38,\n",
    "    \"C3\": 39,\n",
    "    \"C2\": 40,\n",
    "    \"C1\": 41,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vertebrae = \"Th9\"\n",
    "target_size = (64, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No vertebrae found in the segmentation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m coords.shape[\u001b[32m1\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo vertebrae found in the segmentation.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m zmin, ymin, xmin = \u001b[43mcoords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m zmax, ymax, xmax = coords.max(axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PW/INZ/vertabrae_cls/.venv/lib/python3.13/site-packages/numpy/core/_methods.py:45\u001b[39m, in \u001b[36m_amin\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_amin\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     44\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "target_label = VERTEBRAE_MAP.get(target_vertebrae)\n",
    "if target_label is None:\n",
    "    raise ValueError(f\"Invalid target vertebrae: {target_vertebrae}\")\n",
    "\n",
    "labels = [target_label]\n",
    "if target_size is None:\n",
    "    labels.extend([max(18, target_label - 1), min(41, target_label + 1)])\n",
    "\n",
    "mask = np.isin(segmentation, labels).astype(np.uint8)\n",
    "\n",
    "coords = np.array(mask.nonzero())\n",
    "if coords.shape[1] == 0:\n",
    "    print(\"No vertebrae found in the segmentation.\")\n",
    "\n",
    "zmin, ymin, xmin = coords.min(axis=1)\n",
    "zmax, ymax, xmax = coords.max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_coords = np.array((coords.max(axis=1) + coords.min(axis=1)) // 2)\n",
    "z_center, y_center, x_center = target_coords\n",
    "\n",
    "z_half, y_half, x_half = [size // 2 for size in target_size]\n",
    "\n",
    "z_start = z_center - z_half\n",
    "y_start = y_center - y_half\n",
    "x_start = x_center - x_half\n",
    "\n",
    "z_end = z_start + target_size[0]\n",
    "y_end = y_start + target_size[1]\n",
    "x_end = x_start + target_size[2]\n",
    "\n",
    "pad_z_start = max(0, -z_start)\n",
    "pad_y_start = max(0, -y_start)\n",
    "pad_x_start = max(0, -x_start)\n",
    "\n",
    "pad_z_end = max(0, z_end - input_tensor.shape[2])\n",
    "pad_y_end = max(0, y_end - input_tensor.shape[3])\n",
    "pad_x_end = max(0, x_end - input_tensor.shape[4])\n",
    "\n",
    "z_start = max(0, z_start)\n",
    "y_start = max(0, y_start)\n",
    "x_start = max(0, x_start)\n",
    "\n",
    "z_end = min(input_tensor.shape[2], z_end)\n",
    "y_end = min(input_tensor.shape[3], y_end)\n",
    "x_end = min(input_tensor.shape[4], x_end)\n",
    "\n",
    "cropped_tensor = input_tensor[:, :, z_start:z_end, y_start:y_end, x_start:x_end]\n",
    "\n",
    "if any([pad_z_start, pad_y_start, pad_x_start, pad_z_end, pad_y_end, pad_x_end]):\n",
    "    cropped_tensor = torch.nn.functional.pad(\n",
    "        cropped_tensor,\n",
    "        (pad_x_start, pad_x_end, pad_y_start, pad_y_end, pad_z_start, pad_z_end),\n",
    "        mode=\"constant\",\n",
    "        value=0,\n",
    "    )\n",
    "\n",
    "output_tensor = cropped_tensor.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mid = output_tensor.shape[2] // 2\n",
    "plt.imshow(output_tensor[:, :, mid], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Patch dla labelu 27 + sąsiadów: shape=torch.Size([11, 12, 11])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ordered_vertebra_labels = [\n",
    "    41, 40, 39, 38, 37, 36, 35,  # C1–C7\n",
    "    34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23,  # Th1–Th12\n",
    "    22, 21, 20, 19, 18  # L1–L5\n",
    "]\n",
    "\n",
    "def extract_patch_with_neighbors(segmentation, input_tensor, center_label, margin=5, neighbor_range=1):\n",
    "    try:\n",
    "        center_index = ordered_vertebra_labels.index(center_label)\n",
    "    except ValueError:\n",
    "        print(f\"❌ Label {center_label} not in vertebral list.\")\n",
    "        return None\n",
    "\n",
    "    neighbor_labels = ordered_vertebra_labels[\n",
    "        max(0, center_index - neighbor_range) : center_index + neighbor_range + 1\n",
    "    ]\n",
    "\n",
    "    # Maska dla danego kręgu i sąsiadów\n",
    "    mask = np.isin(segmentation, neighbor_labels).astype(np.uint8)\n",
    "\n",
    "    coords = np.array(mask.nonzero())\n",
    "    if coords.shape[1] == 0:\n",
    "        print(\"⚠️ Brak voxeli w masce łączonej.\")\n",
    "        return None\n",
    "\n",
    "    zmin, ymin, xmin = coords.min(axis=1)\n",
    "    zmax, ymax, xmax = coords.max(axis=1)\n",
    "\n",
    "    zmin = max(zmin - margin, 0)\n",
    "    ymin = max(ymin - margin, 0)\n",
    "    xmin = max(xmin - margin, 0)\n",
    "    zmax = min(zmax + margin, segmentation.shape[0])\n",
    "    ymax = min(ymax + margin, segmentation.shape[1])\n",
    "    xmax = min(xmax + margin, segmentation.shape[2])\n",
    "\n",
    "    # Wytnij z input_tensor: (1, 1, D, H, W)\n",
    "    patch = input_tensor[:, :, zmin:zmax, ymin:ymax, xmin:xmax]\n",
    "    patch_tensor = patch.squeeze().detach().cpu()  # (D, H, W)\n",
    "\n",
    "    print(f\"✅ Patch dla labelu {center_label} + sąsiadów: shape={patch_tensor.shape}\")\n",
    "\n",
    "    # Wizualizacja slice Z\n",
    "    mid = patch_tensor.shape[2] // 2\n",
    "    plt.imshow(patch_tensor[:, :, mid], cmap=\"gray\")\n",
    "    plt.title(f\"Kręg {center_label} ±{neighbor_range}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return patch_tensor\n",
    "\n",
    "# Przykład: vertebrae_L1 → label 22\n",
    "patch_L1 = extract_patch_with_neighbors(segmentation, input_tensor, center_label=27)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
