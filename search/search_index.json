{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"vertebrae_cls Documentation","text":""},{"location":"#description","title":"Description","text":"<p>The goal of this project is to create a 3D classifier for spinal injuries. </p> <p>To achieve this, the following steps are performed: - Data is sourced from real-world DICOM files. - The data is preprocessed and adapted to fit the requirements of the classifier. - Vertebrae are segmented using the MONAI WholeBody CT segmentation model, which is used solely for the purpose of extracting individual vertebrae from the scans. - The segmented vertebrae are then used to train and evaluate a custom 3D classification model for spinal injuries.</p> <p>This pipeline ensures that the data is properly prepared for the development of a robust and accurate 3D classifier for spinal injury detection.</p>"},{"location":"commands/","title":"Commands","text":"<p>This document provides an overview of the available <code>make</code> commands for the project. These commands simplify common tasks such as installing dependencies, preparing data, and managing the codebase.</p>"},{"location":"commands/#available-commands","title":"Available Commands","text":""},{"location":"commands/#1-install-python-dependencies","title":"1. Install Python Dependencies","text":"<p>Installs all required Python dependencies for the project.</p> <pre><code>make requirements\n</code></pre>"},{"location":"commands/#2-delete-compiled-python-files","title":"2. Delete Compiled Python Files","text":"<p>Removes all compiled Python files (.pyc, .pyo) and <code>__pycache__</code> directories.</p> <pre><code>make clean\n</code></pre>"},{"location":"commands/#3-lint-the-code","title":"3. Lint the Code","text":"<p>Checks the code for style issues using <code>flake8</code>, <code>isort</code>, and <code>black</code>. Use this command to ensure the code adheres to the project's style guidelines.</p> <pre><code>make lint\n</code></pre>"},{"location":"commands/#4-format-the-code","title":"4. Format the Code","text":"<p>Formats the source code using <code>black</code> according to the configuration in <code>pyproject.toml</code>.</p> <pre><code>make format\n</code></pre>"},{"location":"commands/#5-prepare-interim-data","title":"5. Prepare Interim Data","text":"<p>Processes the raw XLS data and prepares interim data for further processing.</p> <pre><code>make prepare_interim_data\n</code></pre>"},{"location":"commands/#6-create-dataset","title":"6. Create Dataset","text":"<p>Generates the final dataset using the processed interim data.</p> <pre><code>make create_dataset\n</code></pre>"},{"location":"commands/#7-display-help","title":"7. Display Help","text":"<p>Displays a list of all available <code>make</code> commands with their descriptions.</p> <pre><code>make help\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>A guide to setting up the project from scratch, including installing dependencies, preparing data, and creating datasets.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed: - Python version <code>3.13</code> - <code>make</code> utility</p>"},{"location":"getting-started/#setup-instructions","title":"Setup Instructions","text":""},{"location":"getting-started/#clone-the-repository","title":"Clone the repository:","text":"<pre><code>git clone &lt;repository-url&gt;\ncd vertebrae_cls\n</code></pre>"},{"location":"getting-started/#install-python-dependencies-use-the-requirements-command-to-install-all-necessary-python-dependencies","title":"Install Python dependencies: Use the <code>requirements</code> command to install all necessary Python dependencies:","text":"<pre><code>make requirements\n</code></pre>"},{"location":"getting-started/#prepare-interim-data-extract-and-process-data-from-the-xls-file","title":"Prepare interim data: Extract and process data from the XLS file:","text":"<pre><code>make prepare_interim_data\n</code></pre>"},{"location":"getting-started/#create-the-dataset-generate-the-final-dataset-using-the-processed-interim-data","title":"Create the dataset: Generate the final dataset using the processed interim data:","text":"<pre><code>make create_dataset\n</code></pre>"},{"location":"code/data_preparation/","title":"Data preparation","text":""},{"location":"code/data_preparation/#src.data_utils.XLSExtractor.XLSExtractor","title":"<code>XLSExtractor</code>","text":"<p>A class to extract data from XLS files.</p> Source code in <code>src/data_utils/XLSExtractor.py</code> <pre><code>class XLSExtractor:\n    \"\"\"\n    A class to extract data from XLS files.\n    \"\"\"\n\n    def __init__(self, file_path: Path):\n        \"\"\"\n        Initializes the XLSExtractor with the path to the XLS file.\n\n        :param file_path: Path to the XLS file.\n        \"\"\"\n        self.file_path = file_path\n        self.data = None\n        self._load_data()\n        self.records: list[tuple[str, str]] = []\n\n    def _load_data(self) -&gt; None:\n        \"\"\"\n        Loads data from the XLS file into a pandas DataFrame.\n        \"\"\"\n        try:\n            self.data = pd.read_excel(self.file_path)\n        except Exception as e:\n            raise ValueError(f\"Failed to load XLS file: {e}\")\n\n    def _drop_irrelevant_columns(self) -&gt; None:\n        \"\"\"\n        Drops irrelevant columns from the DataFrame.\n        \"\"\"\n        columns_of_interest = [\n            \"I.I.\",\n            \"Poziom\",\n            \"A0\",\n            \"A1\",\n            \"A2\",\n            \"A3\",\n            \"A4\",\n            \"B1\",\n            \"B2\",\n            \"B3\",\n            \"C\",\n        ]\n        self.data = self.data[columns_of_interest].copy()\n\n    def _extract_records(self) -&gt; None:\n        \"\"\"\n        Extracts records from the DataFrame and stores them in the records list.\n        Each record includes patient ID, associated traumas, and starting row index.\n        \"\"\"\n        current_traumas = []\n        current_ii = None\n        current_row_index = None\n\n        for index, row in self.data.iterrows():\n            if pd.notna(row[\"I.I.\"]):\n                if current_traumas:\n                    self.records.append(\n                        {\n                            \"I.I\": f\"{current_ii}\",\n                            \"traumas\": current_traumas,\n                            \"start_row\": current_row_index + 2,\n                        }\n                    )\n                current_traumas = []\n                current_ii = row[\"I.I.\"]\n                current_row_index = index\n\n            if pd.notna(row[\"Poziom\"]):\n                levels = re.split(r\"[/\\-]\", row[\"Poziom\"])\n                for level in levels:\n                    level = level.strip()\n                    for trauma_type in [\"A0\", \"A1\", \"A2\", \"A3\", \"A4\", \"B1\", \"B2\", \"B3\", \"C\"]:\n                        if pd.notna(row[trauma_type]):\n                            current_traumas.append((level, trauma_type))\n\n        if current_traumas:\n            self.records.append(\n                {\n                    \"I.I\": f\"{current_ii}\",\n                    \"traumas\": current_traumas,\n                    \"start_row\": current_row_index + 2,\n                }\n            )\n\n    def extract_and_save(self, output_path: Path) -&gt; None:\n        \"\"\"\n        Extracts records from the XLS file and saves them to a CSV file.\n\n        :param output_path: Path to save the CSV file.\n        \"\"\"\n        self._drop_irrelevant_columns()\n        self._extract_records()\n\n        df = pd.DataFrame(self.records)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        df.to_csv(output_path, index=False)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.XLSExtractor.XLSExtractor.__init__","title":"<code>__init__(file_path)</code>","text":"<p>Initializes the XLSExtractor with the path to the XLS file.</p> <p>:param file_path: Path to the XLS file.</p> Source code in <code>src/data_utils/XLSExtractor.py</code> <pre><code>def __init__(self, file_path: Path):\n    \"\"\"\n    Initializes the XLSExtractor with the path to the XLS file.\n\n    :param file_path: Path to the XLS file.\n    \"\"\"\n    self.file_path = file_path\n    self.data = None\n    self._load_data()\n    self.records: list[tuple[str, str]] = []\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.XLSExtractor.XLSExtractor.extract_and_save","title":"<code>extract_and_save(output_path)</code>","text":"<p>Extracts records from the XLS file and saves them to a CSV file.</p> <p>:param output_path: Path to save the CSV file.</p> Source code in <code>src/data_utils/XLSExtractor.py</code> <pre><code>def extract_and_save(self, output_path: Path) -&gt; None:\n    \"\"\"\n    Extracts records from the XLS file and saves them to a CSV file.\n\n    :param output_path: Path to save the CSV file.\n    \"\"\"\n    self._drop_irrelevant_columns()\n    self._extract_records()\n\n    df = pd.DataFrame(self.records)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_csv(output_path, index=False)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dicom_reader.DICOMReader","title":"<code>DICOMReader</code>","text":"<p>Class to read DICOM files and convert them to PyTorch tensors.</p> Source code in <code>src/data_utils/dicom_reader.py</code> <pre><code>class DICOMReader:\n    \"\"\"\n    Class to read DICOM files and convert them to PyTorch tensors.\n    \"\"\"\n\n    def __init__(self, patient_dir: Path):\n        \"\"\"\n        Initialize the DICOMReader with the directory containing DICOM files.\n\n        Args:\n            patient_dir (Path): Directory containing DICOM files.\n        \"\"\"\n        self.patient_dir = patient_dir\n\n    def _get_image_metadata(self, image: sitk.Image) -&gt; dict:\n        \"\"\"\n        Extract metadata from a SimpleITK image.\n\n        Args:\n            image (sitk.Image): The SimpleITK image object.\n\n        Returns:\n            dict: A dictionary containing the image metadata.\n        \"\"\"\n        metadata = {\n            \"spacing\": image.GetSpacing(),\n            \"direction\": image.GetDirection(),\n            \"origin\": image.GetOrigin(),\n        }\n        return metadata\n\n    def _convert_dicom_to_tensor(self, image: sitk.Image) -&gt; torch.Tensor:\n        \"\"\"\n        Convert a SimpleITK image to a PyTorch tensor.\n\n        Args:\n            image (sitk.Image): The SimpleITK image object.\n\n        Returns:\n            torch.Tensor: The converted PyTorch tensor.\n        \"\"\"\n        image_array = sitk.GetArrayFromImage(image)\n        tensor = torch.tensor(image_array, dtype=torch.float32)\n        return tensor\n\n    def _get_series_description(self, first_file: str) -&gt; str:\n        \"\"\"\n        Extract the series description from the first DICOM file.\n\n        Args:\n            first_file (str): The path to the first DICOM file.\n\n        Returns:\n            str: The series description.\n        \"\"\"\n        file_reader = sitk.ImageFileReader()\n        file_reader.SetFileName(first_file)\n        file_reader.ReadImageInformation()\n        return (\n            file_reader.GetMetaData(\"0008|103e\")\n            if file_reader.HasMetaDataKey(\"0008|103e\")\n            else \"Unknown\"\n        )\n\n    def load_dicom_series_with_metadata(self, series_dir: Path) -&gt; tuple[torch.Tensor, str, dict]:\n        \"\"\"\n        Read a DICOM series from a directory and return the image tensor, series description, and metadata.\n\n        Args:\n            series_dir (str): Path to the directory containing DICOM series.\n\n        Returns:\n            tuple: A tuple containing:\n                - torch.Tensor: The image tensor.\n                - str: The series description.\n                - dict: Metadata including spacing, direction, and origin.\n\n        Raises:\n            ValueError: If no DICOM series is found in the directory.\n        \"\"\"\n        reader = sitk.ImageSeriesReader()\n        dicom_series = reader.GetGDCMSeriesFileNames(series_dir)\n        if not dicom_series:\n            raise ValueError(f\"Did not find any DICOM Series in: {series_dir}\")\n        reader.SetFileNames(dicom_series)\n\n        image = reader.Execute()\n        tensor = self._convert_dicom_to_tensor(image)\n\n        metadata = self._get_image_metadata(image)\n\n        series_description = self._get_series_description(dicom_series[0])\n\n        return tensor, series_description, metadata\n\n    def _load_all_dicom_series(self) -&gt; dict[str, dict[str, torch.Tensor]]:\n        \"\"\"\n        Loads all DICOM series from a patient folder and organizes them by series description.\n\n        Returns:\n            Dict[str, Dict[str, torch.Tensor]]: A dictionary where keys are series descriptions,\n            and values are dictionaries containing tensors and metadata.\n        \"\"\"\n        series_tensors = {}\n\n        for root, dirs, files in os.walk(self.patient_dir):\n            if files:\n                try:\n                    tensor, series_description, metadata = self.load_dicom_series_with_metadata(\n                        root\n                    )\n                    series_tensors[series_description] = {\"tensor\": tensor, \"metadata\": metadata}\n                except Exception:\n                    pass\n\n        return series_tensors\n\n    def _select_tensor_by_priority(\n        self, series_tensors: dict[str, dict[str, torch.Tensor]]\n    ) -&gt; tuple[Optional[torch.Tensor], Optional[str], Optional[dict]]:\n        \"\"\"\n        Selects a tensor based on priority: largest tensor first, then \"kosci\" &gt; \"miekkie\" &gt; rest.\n\n        Args:\n            series_tensors (Dict[str, Dict[str, torch.Tensor]]): Dictionary of series descriptions\n            with tensors and metadata.\n\n        Returns:\n            Tuple[Optional[torch.Tensor], Optional[str], Optional[Dict]]: The selected tensor,\n            its description, and metadata. Returns None if no tensor is found.\n        \"\"\"\n        max_size = 0\n        largest_tensors = []\n\n        for series_description, data in series_tensors.items():\n            tensor_size = data[\"tensor\"].numel()\n            if tensor_size &gt; max_size:\n                max_size = tensor_size\n                largest_tensors = [(series_description, data)]\n            elif tensor_size == max_size:\n                largest_tensors.append((series_description, data))\n\n        for priority in [\"kosci\", \"miekkie\"]:\n            for series_description, data in largest_tensors:\n                if priority in series_description.lower():\n                    return data[\"tensor\"], series_description, data[\"metadata\"]\n\n        if largest_tensors:\n            series_description, data = largest_tensors[0]\n            return data[\"tensor\"], series_description, data[\"metadata\"]\n\n        return None, None, None\n\n    def process_dicom_series(self) -&gt; tuple[Optional[torch.Tensor], Optional[str], Optional[dict]]:\n        \"\"\"\n        Processes DICOM series from a root folder, selects a tensor based on priority.\n\n        Args:\n            dicom_root_folder (str): Path to the root folder containing DICOM series.\n\n        Returns:\n            Tuple[Optional[torch.Tensor], Optional[str], Optional[dict]]: The selected tensor,\n            its description, and metadata. Returns None if no tensor is found.\n\n        Raises:\n            ValueError: If no DICOM series are found in the directory.\n        \"\"\"\n        series_tensors = self._load_all_dicom_series()\n\n        return self._select_tensor_by_priority(series_tensors)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dicom_reader.DICOMReader.__init__","title":"<code>__init__(patient_dir)</code>","text":"<p>Initialize the DICOMReader with the directory containing DICOM files.</p> <p>Parameters:</p> Name Type Description Default <code>patient_dir</code> <code>Path</code> <p>Directory containing DICOM files.</p> required Source code in <code>src/data_utils/dicom_reader.py</code> <pre><code>def __init__(self, patient_dir: Path):\n    \"\"\"\n    Initialize the DICOMReader with the directory containing DICOM files.\n\n    Args:\n        patient_dir (Path): Directory containing DICOM files.\n    \"\"\"\n    self.patient_dir = patient_dir\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dicom_reader.DICOMReader.load_dicom_series_with_metadata","title":"<code>load_dicom_series_with_metadata(series_dir)</code>","text":"<p>Read a DICOM series from a directory and return the image tensor, series description, and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>series_dir</code> <code>str</code> <p>Path to the directory containing DICOM series.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, str, dict]</code> <p>A tuple containing: - torch.Tensor: The image tensor. - str: The series description. - dict: Metadata including spacing, direction, and origin.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no DICOM series is found in the directory.</p> Source code in <code>src/data_utils/dicom_reader.py</code> <pre><code>def load_dicom_series_with_metadata(self, series_dir: Path) -&gt; tuple[torch.Tensor, str, dict]:\n    \"\"\"\n    Read a DICOM series from a directory and return the image tensor, series description, and metadata.\n\n    Args:\n        series_dir (str): Path to the directory containing DICOM series.\n\n    Returns:\n        tuple: A tuple containing:\n            - torch.Tensor: The image tensor.\n            - str: The series description.\n            - dict: Metadata including spacing, direction, and origin.\n\n    Raises:\n        ValueError: If no DICOM series is found in the directory.\n    \"\"\"\n    reader = sitk.ImageSeriesReader()\n    dicom_series = reader.GetGDCMSeriesFileNames(series_dir)\n    if not dicom_series:\n        raise ValueError(f\"Did not find any DICOM Series in: {series_dir}\")\n    reader.SetFileNames(dicom_series)\n\n    image = reader.Execute()\n    tensor = self._convert_dicom_to_tensor(image)\n\n    metadata = self._get_image_metadata(image)\n\n    series_description = self._get_series_description(dicom_series[0])\n\n    return tensor, series_description, metadata\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dicom_reader.DICOMReader.process_dicom_series","title":"<code>process_dicom_series()</code>","text":"<p>Processes DICOM series from a root folder, selects a tensor based on priority.</p> <p>Parameters:</p> Name Type Description Default <code>dicom_root_folder</code> <code>str</code> <p>Path to the root folder containing DICOM series.</p> required <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Tuple[Optional[torch.Tensor], Optional[str], Optional[dict]]: The selected tensor,</p> <code>Optional[str]</code> <p>its description, and metadata. Returns None if no tensor is found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no DICOM series are found in the directory.</p> Source code in <code>src/data_utils/dicom_reader.py</code> <pre><code>def process_dicom_series(self) -&gt; tuple[Optional[torch.Tensor], Optional[str], Optional[dict]]:\n    \"\"\"\n    Processes DICOM series from a root folder, selects a tensor based on priority.\n\n    Args:\n        dicom_root_folder (str): Path to the root folder containing DICOM series.\n\n    Returns:\n        Tuple[Optional[torch.Tensor], Optional[str], Optional[dict]]: The selected tensor,\n        its description, and metadata. Returns None if no tensor is found.\n\n    Raises:\n        ValueError: If no DICOM series are found in the directory.\n    \"\"\"\n    series_tensors = self._load_all_dicom_series()\n\n    return self._select_tensor_by_priority(series_tensors)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.vertebra_extractor.VertebraExtractor","title":"<code>VertebraExtractor</code>","text":"<p>A class to extract specific vertebrae from CT images.</p> Source code in <code>src/data_utils/vertebra_extractor.py</code> <pre><code>class VertebraExtractor:\n    \"\"\"\n    A class to extract specific vertebrae from CT images.\n    \"\"\"\n\n    def __init__(\n        self, is_full_resolution: bool = False, device: torch.device = torch.device(\"cuda\")\n    ):\n        \"\"\"\n        Initialize the VertebraExtractor.\n\n        Args:\n            is_full_resolution (bool): If True, use full resolution model.\n            device (torch.device): The device to run the model on.\n        \"\"\"\n        pixdim = (1.5, 1.5, 1.5) if is_full_resolution else (3.0, 3.0, 3.0)\n        self.transforms = Compose(\n            [\n                LoadImaged(keys=[\"image\"]),\n                EnsureTyped(keys=[\"image\"]),\n                EnsureChannelFirstd(keys=[\"image\"]),\n                Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n                Spacingd(keys=[\"image\"], pixdim=pixdim, mode=\"bilinear\"),\n                NormalizeIntensityd(keys=[\"image\"], nonzero=True),\n                ScaleIntensityd(keys=[\"image\"], minv=-1.0, maxv=1.0),\n            ]\n        )\n        self.device = device\n        self.is_full_resolution = is_full_resolution\n        self.model = self._load_model()\n\n        self.segmentation: torch.Tensor | None = None\n        self.transformed_input_tensor: torch.Tensor | None = None\n\n    def _load_model(self) -&gt; torch.nn.Module:\n        \"\"\"\n        Load the vertebra extraction model.\n\n        Returns:\n            torch.nn.Module: The loaded model.\n        \"\"\"\n\n        model_filename = \"model.pt\" if self.is_full_resolution else \"model_lowres.pt\"\n        model_path = SEG_MODEL_DIR / \"models\" / model_filename\n        config_path = SEG_MODEL_DIR / \"configs\" / \"inference.json\"\n\n        parser = ConfigParser()\n        parser.read_config(config_path)\n\n        model = parser.get_parsed_content(\"network_def\")\n        weights = torch.load(model_path, map_location=self.device)\n\n        if isinstance(weights, dict) and \"state_dict\" in weights:\n            weights = weights[\"state_dict\"]\n        model.load_state_dict(weights)\n        model.eval()\n        model.to(self.device)\n        return model\n\n    def _save_tensor_as_nrrd(self, tensor: torch.Tensor, metadata: dict, filename: str) -&gt; None:\n        \"\"\"\n        Save a PyTorch tensor as an NRRD file with metadata.\n\n        Args:\n            tensor (torch.Tensor): The tensor to save.\n            metadata (dict): Metadata to include in the NRRD file.\\\n            filename (str): The filename to save the NRRD file as.\n        \"\"\"\n        array = tensor.numpy()\n\n        image = sitk.GetImageFromArray(array)\n\n        if \"spacing\" in metadata:\n            image.SetSpacing(metadata[\"spacing\"])\n        if \"direction\" in metadata:\n            image.SetDirection(metadata[\"direction\"])\n        if \"origin\" in metadata:\n            image.SetOrigin(metadata[\"origin\"])\n\n        sitk.WriteImage(image, filename)\n\n    def _transform_data(self, tensor: torch.Tensor, metadata: dict) -&gt; torch.Tensor:\n        \"\"\"\n        Transform the input tensor data.\n\n        Args:\n            tensor (torch.Tensor): The input tensor.\n            metadata (dict): Metadata for the tensor.\n\n        Returns:\n            torch.Tensor: The transformed tensor.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_file = os.path.join(temp_dir, \"output.nrrd\")\n\n            self._save_tensor_as_nrrd(tensor, metadata, temp_file)\n\n            data = {\"image\": temp_file}\n\n            transformed_data = self.transforms(data)\n            return transformed_data[\"image\"]\n\n    def get_segmentation(\n        self, raw_tensor: torch.Tensor, metadata: dict\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get the segmentation of the input tensor.\n\n        Args:\n            raw_tensor (torch.Tensor): The input tensor.\n            metadata (dict): Metadata for the tensor.\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor]: The segmentation tensor and the input tensor.\n        \"\"\"\n        transformed_tensor = self._transform_data(raw_tensor, metadata)\n\n        with torch.inference_mode():\n            input_tensor = transformed_tensor.unsqueeze(0).to(self.device)\n            output = sliding_window_inference(\n                inputs=input_tensor,\n                roi_size=(96, 96, 96),\n                sw_batch_size=1,\n                predictor=self.model,\n                overlap=0.5,\n                mode=\"gaussian\",\n            )\n            output = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n\n        return output, input_tensor\n\n    def _resize_vertebrae_tensor(\n        self,\n        input_tensor: torch.Tensor,\n        coords: np.ndarray,\n        target_size: tuple[int, int, int],\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Resize the vertebrae tensor to the target size, applying padding if necessary.\n\n        Args:\n            input_tensor (torch.Tensor): The input tensor.\n            coords (np.ndarray): Coordinates of the vertebrae.\n            target_size (tuple[int, int, int]): Target size for resizing.\n\n        Returns:\n            torch.Tensor: The resized tensor with padding if required.\n        \"\"\"\n        target_coords = np.array((coords.max(axis=1) + coords.min(axis=1)) // 2)\n        z_center, y_center, x_center = target_coords\n\n        z_half, y_half, x_half = [size // 2 for size in target_size]\n\n        z_start = z_center - z_half\n        y_start = y_center - y_half\n        x_start = x_center - x_half\n\n        z_end = z_start + target_size[0]\n        y_end = y_start + target_size[1]\n        x_end = x_start + target_size[2]\n\n        pad_z_start = max(0, -z_start)\n        pad_y_start = max(0, -y_start)\n        pad_x_start = max(0, -x_start)\n\n        pad_z_end = max(0, z_end - input_tensor.shape[2])\n        pad_y_end = max(0, y_end - input_tensor.shape[3])\n        pad_x_end = max(0, x_end - input_tensor.shape[4])\n\n        z_start = max(0, z_start)\n        y_start = max(0, y_start)\n        x_start = max(0, x_start)\n\n        z_end = min(input_tensor.shape[2], z_end)\n        y_end = min(input_tensor.shape[3], y_end)\n        x_end = min(input_tensor.shape[4], x_end)\n\n        cropped_tensor = input_tensor[:, :, z_start:z_end, y_start:y_end, x_start:x_end]\n\n        if any([pad_z_start, pad_y_start, pad_x_start, pad_z_end, pad_y_end, pad_x_end]):\n            cropped_tensor = torch.nn.functional.pad(\n                cropped_tensor,\n                (pad_x_start, pad_x_end, pad_y_start, pad_y_end, pad_z_start, pad_z_end),\n                mode=\"constant\",\n                value=0,\n            )\n\n        return cropped_tensor.detach().cpu()\n\n    def extract_vertebrae_with_neighbors(\n        self,\n        input_tensor: torch.Tensor,\n        metadata: dict,\n        target_vertebrae: str,\n        target_size: Optional[tuple[int, int, int]] = None,\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"\n        Extract the specified vertebrae and its neighbors from the input tensor.\n\n        Args:\n            input_tensor (torch.Tensor): The input tensor.\n            metadata (dict): Metadata for the tensor.\n            target_vertebrae (str): The vertebrae to extract.\n            target_size (tuple[int, int, int], optional): The target size for resizing.\n                If None, the original size will be used.\n\n        Returns:\n            Optional[torch.Tensor]: The extracted vertebrae tensor, or None if no vertebrae are found.\n        \"\"\"\n        target_label = VERTEBRAE_MAP.get(target_vertebrae)\n        if target_label is None:\n            raise ValueError(f\"Invalid target vertebrae: {target_vertebrae}\")\n\n        labels = [target_label]\n        if target_size is None:\n            labels.extend([max(18, target_label - 1), min(41, target_label + 1)])\n\n        if self.segmentation is None or self.transformed_input_tensor is None:\n            self.segmentation, self.transformed_input_tensor = self.get_segmentation(\n                input_tensor, metadata\n            )\n\n        mask = np.isin(self.segmentation, labels).astype(np.uint8)\n\n        coords = np.array(mask.nonzero())\n        if coords.shape[1] == 0:\n            print(\"No vertebrae found in the segmentation.\")\n            return None\n\n        zmin, ymin, xmin = coords.min(axis=1)\n        zmax, ymax, xmax = coords.max(axis=1)\n\n        if target_size is None:\n            output = self.transformed_input_tensor[:, :, zmin:zmax, ymin:ymax, xmin:xmax]\n            return output.squeeze().detach().cpu()\n\n        return self._resize_vertebrae_tensor(self.transformed_input_tensor, coords, target_size)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.vertebra_extractor.VertebraExtractor.__init__","title":"<code>__init__(is_full_resolution=False, device=torch.device('cuda'))</code>","text":"<p>Initialize the VertebraExtractor.</p> <p>Parameters:</p> Name Type Description Default <code>is_full_resolution</code> <code>bool</code> <p>If True, use full resolution model.</p> <code>False</code> <code>device</code> <code>device</code> <p>The device to run the model on.</p> <code>device('cuda')</code> Source code in <code>src/data_utils/vertebra_extractor.py</code> <pre><code>def __init__(\n    self, is_full_resolution: bool = False, device: torch.device = torch.device(\"cuda\")\n):\n    \"\"\"\n    Initialize the VertebraExtractor.\n\n    Args:\n        is_full_resolution (bool): If True, use full resolution model.\n        device (torch.device): The device to run the model on.\n    \"\"\"\n    pixdim = (1.5, 1.5, 1.5) if is_full_resolution else (3.0, 3.0, 3.0)\n    self.transforms = Compose(\n        [\n            LoadImaged(keys=[\"image\"]),\n            EnsureTyped(keys=[\"image\"]),\n            EnsureChannelFirstd(keys=[\"image\"]),\n            Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n            Spacingd(keys=[\"image\"], pixdim=pixdim, mode=\"bilinear\"),\n            NormalizeIntensityd(keys=[\"image\"], nonzero=True),\n            ScaleIntensityd(keys=[\"image\"], minv=-1.0, maxv=1.0),\n        ]\n    )\n    self.device = device\n    self.is_full_resolution = is_full_resolution\n    self.model = self._load_model()\n\n    self.segmentation: torch.Tensor | None = None\n    self.transformed_input_tensor: torch.Tensor | None = None\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.vertebra_extractor.VertebraExtractor.extract_vertebrae_with_neighbors","title":"<code>extract_vertebrae_with_neighbors(input_tensor, metadata, target_vertebrae, target_size=None)</code>","text":"<p>Extract the specified vertebrae and its neighbors from the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>metadata</code> <code>dict</code> <p>Metadata for the tensor.</p> required <code>target_vertebrae</code> <code>str</code> <p>The vertebrae to extract.</p> required <code>target_size</code> <code>tuple[int, int, int]</code> <p>The target size for resizing. If None, the original size will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>Optional[torch.Tensor]: The extracted vertebrae tensor, or None if no vertebrae are found.</p> Source code in <code>src/data_utils/vertebra_extractor.py</code> <pre><code>def extract_vertebrae_with_neighbors(\n    self,\n    input_tensor: torch.Tensor,\n    metadata: dict,\n    target_vertebrae: str,\n    target_size: Optional[tuple[int, int, int]] = None,\n) -&gt; Optional[torch.Tensor]:\n    \"\"\"\n    Extract the specified vertebrae and its neighbors from the input tensor.\n\n    Args:\n        input_tensor (torch.Tensor): The input tensor.\n        metadata (dict): Metadata for the tensor.\n        target_vertebrae (str): The vertebrae to extract.\n        target_size (tuple[int, int, int], optional): The target size for resizing.\n            If None, the original size will be used.\n\n    Returns:\n        Optional[torch.Tensor]: The extracted vertebrae tensor, or None if no vertebrae are found.\n    \"\"\"\n    target_label = VERTEBRAE_MAP.get(target_vertebrae)\n    if target_label is None:\n        raise ValueError(f\"Invalid target vertebrae: {target_vertebrae}\")\n\n    labels = [target_label]\n    if target_size is None:\n        labels.extend([max(18, target_label - 1), min(41, target_label + 1)])\n\n    if self.segmentation is None or self.transformed_input_tensor is None:\n        self.segmentation, self.transformed_input_tensor = self.get_segmentation(\n            input_tensor, metadata\n        )\n\n    mask = np.isin(self.segmentation, labels).astype(np.uint8)\n\n    coords = np.array(mask.nonzero())\n    if coords.shape[1] == 0:\n        print(\"No vertebrae found in the segmentation.\")\n        return None\n\n    zmin, ymin, xmin = coords.min(axis=1)\n    zmax, ymax, xmax = coords.max(axis=1)\n\n    if target_size is None:\n        output = self.transformed_input_tensor[:, :, zmin:zmax, ymin:ymax, xmin:xmax]\n        return output.squeeze().detach().cpu()\n\n    return self._resize_vertebrae_tensor(self.transformed_input_tensor, coords, target_size)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.vertebra_extractor.VertebraExtractor.get_segmentation","title":"<code>get_segmentation(raw_tensor, metadata)</code>","text":"<p>Get the segmentation of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>raw_tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>metadata</code> <code>dict</code> <p>Metadata for the tensor.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor]: The segmentation tensor and the input tensor.</p> Source code in <code>src/data_utils/vertebra_extractor.py</code> <pre><code>def get_segmentation(\n    self, raw_tensor: torch.Tensor, metadata: dict\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Get the segmentation of the input tensor.\n\n    Args:\n        raw_tensor (torch.Tensor): The input tensor.\n        metadata (dict): Metadata for the tensor.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: The segmentation tensor and the input tensor.\n    \"\"\"\n    transformed_tensor = self._transform_data(raw_tensor, metadata)\n\n    with torch.inference_mode():\n        input_tensor = transformed_tensor.unsqueeze(0).to(self.device)\n        output = sliding_window_inference(\n            inputs=input_tensor,\n            roi_size=(96, 96, 96),\n            sw_batch_size=1,\n            predictor=self.model,\n            overlap=0.5,\n            mode=\"gaussian\",\n        )\n        output = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n\n    return output, input_tensor\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator","title":"<code>DatasetCreator</code>","text":"<p>Class to create datasets for vertebrae segmentation.</p> Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>class DatasetCreator:\n    \"\"\"\n    Class to create datasets for vertebrae segmentation.\n    \"\"\"\n\n    def __init__(self, raport_file_path: Path) -&gt; None:\n        \"\"\"\n        Initialize the DatasetCreator.\n\n        Args:\n            raport_file_path (Path): Path to raport file.\n        \"\"\"\n        self.raport_file_path = raport_file_path\n\n    def _process_injured_vertebrae(\n        self,\n        vertebra_extractor: VertebraExtractor,\n        tensor: torch.Tensor,\n        metadata: dict,\n        dir_name: str,\n        injuried_vertebrae: list[tuple[str, str]],\n        target_size: Optional[tuple[int, int, int]],\n    ) -&gt; list[dict]:\n        \"\"\"\n        Process injured vertebrae and extract their tensors.\n\n        Args:\n            vertebra_extractor (VertebraExtractor): The vertebra extractor instance.\n            tensor (torch.Tensor): The input tensor.\n            metadata (dict): Metadata for the tensor.\n            dir_name (str): Directory name of the patient.\n            injuried_vertebrae (list[tuple[str, str]]): List of tuples containing vertebrae information (vertebra, injury_type).\n            target_size (tuple[int, int, int], optional): Target size for the extracted tensors.\n\n        Returns:\n            list[dict]: List of dictionaries containing injured vertebrae data.\n        \"\"\"\n        injured_data = []\n        for vertebra, injury_type in injuried_vertebrae:\n            try:\n                target_tensor = vertebra_extractor.extract_vertebrae_with_neighbors(\n                    input_tensor=tensor,\n                    metadata=metadata,\n                    target_vertebrae=vertebra,\n                    target_size=target_size,\n                )\n                if target_tensor is not None:\n                    injured_data.append(\n                        {\n                            \"vertebra\": vertebra,\n                            \"injury_type\": injury_type,\n                            \"target_tensor\": target_tensor,\n                            \"II\": dir_name,\n                        }\n                    )\n            except Exception as e:\n                logger.error(f\"Error ijuried processing {vertebra} in {dir_name}: {e}\")\n\n        return injured_data\n\n    def _process_healthy_vertebrae(\n        self,\n        vertebra_extractor: VertebraExtractor,\n        tensor: torch.Tensor,\n        metadata: dict,\n        dir_name: str,\n        injuried_vertebrae: list[tuple[str, str]],\n        target_size: Optional[tuple[int, int, int]],\n        num_healthy: int,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Process healthy vertebrae and extract their tensors.\n\n        Args:\n            vertebra_extractor (VertebraExtractor): The vertebra extractor instance.\n            tensor (torch.Tensor): The input tensor.\n            metadata (dict): Metadata for the tensor.\n            dir_name (str): Directory name of the patient.\n            injuried_vertebrae (list[tuple[str, str]]): List of tuples containing vertebrae information (vertebra, injury_type).\n            target_size (tuple[int, int, int], optional): Target size for the extracted tensors.\n            num_healthy (int): Number of healthy vertebrae to extract.\n\n        Returns:\n            list[dict]: List of dictionaries containing healthy vertebrae data.\n        \"\"\"\n        healthy_data = []\n        all_vertebrae = set(VERTEBRAE_MAP.keys())\n        injured_vertebrae_names = {vertebra for vertebra, _ in injuried_vertebrae}\n        healthy_vertebrae = list(all_vertebrae - injured_vertebrae_names)\n\n        random.shuffle(healthy_vertebrae)\n\n        for vertebra in healthy_vertebrae:\n            if len(healthy_data) &gt;= num_healthy:\n                break\n\n            try:\n                target_tensor = vertebra_extractor.extract_vertebrae_with_neighbors(\n                    input_tensor=tensor,\n                    metadata=metadata,\n                    target_vertebrae=vertebra,\n                    target_size=target_size,\n                )\n                if target_tensor is not None:\n                    healthy_data.append(\n                        {\n                            \"vertebra\": vertebra,\n                            \"injury_type\": \"H\",\n                            \"target_tensor\": target_tensor,\n                            \"II\": dir_name,\n                        }\n                    )\n            except Exception as e:\n                logger.error(f\"Error processing healthy {vertebra} in {dir_name}: {e}\")\n\n        if len(healthy_data) &lt; num_healthy:\n            logger.warning(\n                f\"Only {len(healthy_data)} healthy vertebrae extracted out of {num_healthy} requested for patient {dir_name}.\"\n            )\n\n        return healthy_data\n\n    def process_patient(\n        self,\n        dir_name: str,\n        injuried_vertebrae: list[tuple[str, str]],\n        target_size: Optional[tuple[int, int, int]] = None,\n        num_healthy: int = 1,\n    ) -&gt; list[dict]:\n        \"\"\"\n        Process a patient's DICOM files and extract vertebrae data.\n        This function reads the DICOM files, extracts the vertebrae and their injuries,\n        and returns a list of dictionaries containing the vertebrae data.\n        Additionally, it extracts a specified number of random healthy vertebrae not listed as injured.\n\n        Args:\n            dir_name (str): Directory name of the patient.\n            injuried_vertebrae (list[tuple[str, str]]): List of tuples containing vertebrae information (vertebra, injury_type).\n            target_size (tuple[int, int, int], optional): Target size for the extracted tensors.\n            num_healthy (int): Number of healthy vertebrae to extract.\n\n        Returns:\n            list[dict]: List of dictionaries containing vertebrae data.\n        \"\"\"\n        patient_data = []\n        dir_path = DICOM_DATA_DIR / dir_name\n        dicom_reader = DICOMReader(dir_path)\n        tensor, description, metadata = dicom_reader.process_dicom_series()\n        vertebra_extractor = VertebraExtractor(IS_FULL_RESOLUTION, DEVICE)\n\n        patient_data.extend(\n            self._process_injured_vertebrae(\n                vertebra_extractor, tensor, metadata, dir_name, injuried_vertebrae, target_size\n            )\n        )\n\n        patient_data.extend(\n            self._process_healthy_vertebrae(\n                vertebra_extractor,\n                tensor,\n                metadata,\n                dir_name,\n                injuried_vertebrae,\n                target_size,\n                num_healthy,\n            )\n        )\n\n        return patient_data\n\n    def extract_dir_name(self, dir_id: str) -&gt; str:\n        \"\"\"\n        Finds a single directory in DICOM_DATA_DIR whose name contains the given dir_id\n        as the second space-separated component.\n\n        Args:\n            dir_id (str): The directory ID to match (e.g. '303').\n\n        Returns:\n            str: The matching directory name.\n\n        Raises:\n            FileNotFoundError: If no matching directory is found.\n        \"\"\"\n        for path in DICOM_DATA_DIR.iterdir():\n            if not path.is_dir():\n                continue\n            parts = path.name.split()\n            if len(parts) &gt;= 2 and parts[1] == dir_id:\n                return str(path.name)\n        raise FileNotFoundError(\n            f\"No directory found in {DICOM_DATA_DIR} with ID '{dir_id}' as second part.\"\n        )\n\n    def get_max_index(self) -&gt; int:\n        \"\"\"\n        Gets max existing tensor index.\n\n        Returns:\n            int: max existing tensor index.\n        \"\"\"\n        existing_files = list(TENSOR_DIR.glob(\"*.pt\"))\n        if existing_files:\n            max_index = max(int(f.stem) for f in existing_files if f.stem.isdigit())\n            return max_index\n        return 0\n\n    def save_patient_data(self, patient_data: list[dict]) -&gt; None:\n        \"\"\"\n        Save processed vertebra data as tensors and append metadata to a CSV file.\n\n        Each tensor is saved to `TENSOR_DIR` using a sequential 5-digit filename (e.g., \"00001.pt\").\n        Corresponding metadata including vertebra name, injury type, patient ID (II), and file path\n        is appended to the CSV file at `LABELS_FILE_PATH`. If the CSV does not exist, it is created.\n\n        This function is designed to support resumable dataset creation \u2014 partial results are not lost\n        in case of interruption.\n\n        Args:\n            patient_data (list[dict]):\n                A list of dictionaries, where each dictionary contains:\n                - \"target_tensor\" (torch.Tensor): The extracted vertebra tensor.\n                - \"vertebra\" (str): Name of the vertebra (e.g., \"L1\").\n                - \"injury_type\" (str): Injury classification (e.g., \"A1\").\n                - \"II\" (str): Patient ID.\n        \"\"\"\n        labels_file_exists = LABELS_FILE_PATH.exists()\n        next_index = self.get_max_index() + 1\n\n        with open(LABELS_FILE_PATH, mode=\"a\", newline=\"\") as csvfile:\n            writer = csv.DictWriter(\n                csvfile, fieldnames=[\"vertebra\", \"injury_type\", \"II\", \"tensor_path\"]\n            )\n\n            if not labels_file_exists:\n                writer.writeheader()\n\n            for entry in patient_data:\n                tensor = entry[\"target_tensor\"]\n                vertebra = entry[\"vertebra\"]\n                injury_type = entry[\"injury_type\"]\n                II = entry[\"II\"]\n\n                tensor_file_path = TENSOR_DIR / f\"{next_index:05d}.pt\"\n\n                TENSOR_DIR.mkdir(parents=True, exist_ok=True)\n\n                torch.save(tensor, tensor_file_path)\n\n                writer.writerow(\n                    {\n                        \"vertebra\": vertebra,\n                        \"injury_type\": injury_type,\n                        \"II\": II,\n                        \"tensor_path\": str(f\"{next_index:05d}\"),\n                    }\n                )\n                next_index += 1\n\n    def _get_processed_patients(self) -&gt; set[str]:\n        \"\"\"\n        Get processed patients from the labels file.\n\n        Returns:\n            set[str]: Set of processed patient identifiers.\n        \"\"\"\n        processed_patients = set()\n        if LABELS_FILE_PATH.exists():\n            with open(LABELS_FILE_PATH, newline=\"\") as csvfile:\n                reader = csv.DictReader(csvfile)\n                for row in reader:\n                    processed_patients.add(row[\"II\"])\n        return processed_patients\n\n    def create_dataset(\n        self,\n        target_size: Optional[tuple[int, int, int]] = None,\n        num_healthy: int = 1,\n        ignore_existing: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Parse the report file and process DICOM data for each patient.\n\n        This function reads a CSV file where each row contains:\n            - a patient directory identifier (may represent multiple patients),\n            - a list of injured vertebrae with injury types.\n\n        It extracts vertebra data for each patient and saves it using `process_patient`\n        and `save_patient_data`. If any part of the process fails for a patient,\n        others will still be processed.\n\n        Args:\n            target_size (Optional[tuple[int, int, int]]):\n                Optional shape to which extracted vertebra tensors should be resized.\n                If None, original size is preserved.\n            num_healthy (int): Number of healthy vertebrae to extract.\n            ignore_existing (bool): If True, existing extracted data will be deleted.\n\n        \"\"\"\n        if ignore_existing:\n            if LABELS_FILE_PATH.exists():\n                LABELS_FILE_PATH.unlink()\n            if TENSOR_DIR.exists():\n                shutil.rmtree(TENSOR_DIR)\n                print(f\"Deleted contents of tensor directory: {TENSOR_DIR}\")\n\n        processed_patients = self._get_processed_patients()\n\n        with open(self.raport_file_path, newline=\"\") as f:\n            reader = csv.reader(f)\n            next(reader)\n            for row in tqdm(reader):\n                _, raw_injuries, dir_id = row\n                logger.info(f\"Processing dir id:{dir_id}.\")\n                try:\n                    dir_name = self.extract_dir_name(dir_id)\n                except FileNotFoundError as e:\n                    logger.error(e)\n                    continue\n                if dir_name in processed_patients:\n                    continue\n                injuries = ast.literal_eval(raw_injuries)\n\n                patient_data = self.process_patient(\n                    dir_name=dir_name,\n                    injuried_vertebrae=injuries,\n                    target_size=target_size,\n                    num_healthy=num_healthy,\n                )\n                self.save_patient_data(patient_data)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator.__init__","title":"<code>__init__(raport_file_path)</code>","text":"<p>Initialize the DatasetCreator.</p> <p>Parameters:</p> Name Type Description Default <code>raport_file_path</code> <code>Path</code> <p>Path to raport file.</p> required Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>def __init__(self, raport_file_path: Path) -&gt; None:\n    \"\"\"\n    Initialize the DatasetCreator.\n\n    Args:\n        raport_file_path (Path): Path to raport file.\n    \"\"\"\n    self.raport_file_path = raport_file_path\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator.create_dataset","title":"<code>create_dataset(target_size=None, num_healthy=1, ignore_existing=False)</code>","text":"<p>Parse the report file and process DICOM data for each patient.</p> This function reads a CSV file where each row contains <ul> <li>a patient directory identifier (may represent multiple patients),</li> <li>a list of injured vertebrae with injury types.</li> </ul> <p>It extracts vertebra data for each patient and saves it using <code>process_patient</code> and <code>save_patient_data</code>. If any part of the process fails for a patient, others will still be processed.</p> <p>Parameters:</p> Name Type Description Default <code>target_size</code> <code>Optional[tuple[int, int, int]]</code> <p>Optional shape to which extracted vertebra tensors should be resized. If None, original size is preserved.</p> <code>None</code> <code>num_healthy</code> <code>int</code> <p>Number of healthy vertebrae to extract.</p> <code>1</code> <code>ignore_existing</code> <code>bool</code> <p>If True, existing extracted data will be deleted.</p> <code>False</code> Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>def create_dataset(\n    self,\n    target_size: Optional[tuple[int, int, int]] = None,\n    num_healthy: int = 1,\n    ignore_existing: bool = False,\n) -&gt; None:\n    \"\"\"\n    Parse the report file and process DICOM data for each patient.\n\n    This function reads a CSV file where each row contains:\n        - a patient directory identifier (may represent multiple patients),\n        - a list of injured vertebrae with injury types.\n\n    It extracts vertebra data for each patient and saves it using `process_patient`\n    and `save_patient_data`. If any part of the process fails for a patient,\n    others will still be processed.\n\n    Args:\n        target_size (Optional[tuple[int, int, int]]):\n            Optional shape to which extracted vertebra tensors should be resized.\n            If None, original size is preserved.\n        num_healthy (int): Number of healthy vertebrae to extract.\n        ignore_existing (bool): If True, existing extracted data will be deleted.\n\n    \"\"\"\n    if ignore_existing:\n        if LABELS_FILE_PATH.exists():\n            LABELS_FILE_PATH.unlink()\n        if TENSOR_DIR.exists():\n            shutil.rmtree(TENSOR_DIR)\n            print(f\"Deleted contents of tensor directory: {TENSOR_DIR}\")\n\n    processed_patients = self._get_processed_patients()\n\n    with open(self.raport_file_path, newline=\"\") as f:\n        reader = csv.reader(f)\n        next(reader)\n        for row in tqdm(reader):\n            _, raw_injuries, dir_id = row\n            logger.info(f\"Processing dir id:{dir_id}.\")\n            try:\n                dir_name = self.extract_dir_name(dir_id)\n            except FileNotFoundError as e:\n                logger.error(e)\n                continue\n            if dir_name in processed_patients:\n                continue\n            injuries = ast.literal_eval(raw_injuries)\n\n            patient_data = self.process_patient(\n                dir_name=dir_name,\n                injuried_vertebrae=injuries,\n                target_size=target_size,\n                num_healthy=num_healthy,\n            )\n            self.save_patient_data(patient_data)\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator.extract_dir_name","title":"<code>extract_dir_name(dir_id)</code>","text":"<p>Finds a single directory in DICOM_DATA_DIR whose name contains the given dir_id as the second space-separated component.</p> <p>Parameters:</p> Name Type Description Default <code>dir_id</code> <code>str</code> <p>The directory ID to match (e.g. '303').</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The matching directory name.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching directory is found.</p> Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>def extract_dir_name(self, dir_id: str) -&gt; str:\n    \"\"\"\n    Finds a single directory in DICOM_DATA_DIR whose name contains the given dir_id\n    as the second space-separated component.\n\n    Args:\n        dir_id (str): The directory ID to match (e.g. '303').\n\n    Returns:\n        str: The matching directory name.\n\n    Raises:\n        FileNotFoundError: If no matching directory is found.\n    \"\"\"\n    for path in DICOM_DATA_DIR.iterdir():\n        if not path.is_dir():\n            continue\n        parts = path.name.split()\n        if len(parts) &gt;= 2 and parts[1] == dir_id:\n            return str(path.name)\n    raise FileNotFoundError(\n        f\"No directory found in {DICOM_DATA_DIR} with ID '{dir_id}' as second part.\"\n    )\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator.get_max_index","title":"<code>get_max_index()</code>","text":"<p>Gets max existing tensor index.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>max existing tensor index.</p> Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>def get_max_index(self) -&gt; int:\n    \"\"\"\n    Gets max existing tensor index.\n\n    Returns:\n        int: max existing tensor index.\n    \"\"\"\n    existing_files = list(TENSOR_DIR.glob(\"*.pt\"))\n    if existing_files:\n        max_index = max(int(f.stem) for f in existing_files if f.stem.isdigit())\n        return max_index\n    return 0\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator.process_patient","title":"<code>process_patient(dir_name, injuried_vertebrae, target_size=None, num_healthy=1)</code>","text":"<p>Process a patient's DICOM files and extract vertebrae data. This function reads the DICOM files, extracts the vertebrae and their injuries, and returns a list of dictionaries containing the vertebrae data. Additionally, it extracts a specified number of random healthy vertebrae not listed as injured.</p> <p>Parameters:</p> Name Type Description Default <code>dir_name</code> <code>str</code> <p>Directory name of the patient.</p> required <code>injuried_vertebrae</code> <code>list[tuple[str, str]]</code> <p>List of tuples containing vertebrae information (vertebra, injury_type).</p> required <code>target_size</code> <code>tuple[int, int, int]</code> <p>Target size for the extracted tensors.</p> <code>None</code> <code>num_healthy</code> <code>int</code> <p>Number of healthy vertebrae to extract.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of dictionaries containing vertebrae data.</p> Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>def process_patient(\n    self,\n    dir_name: str,\n    injuried_vertebrae: list[tuple[str, str]],\n    target_size: Optional[tuple[int, int, int]] = None,\n    num_healthy: int = 1,\n) -&gt; list[dict]:\n    \"\"\"\n    Process a patient's DICOM files and extract vertebrae data.\n    This function reads the DICOM files, extracts the vertebrae and their injuries,\n    and returns a list of dictionaries containing the vertebrae data.\n    Additionally, it extracts a specified number of random healthy vertebrae not listed as injured.\n\n    Args:\n        dir_name (str): Directory name of the patient.\n        injuried_vertebrae (list[tuple[str, str]]): List of tuples containing vertebrae information (vertebra, injury_type).\n        target_size (tuple[int, int, int], optional): Target size for the extracted tensors.\n        num_healthy (int): Number of healthy vertebrae to extract.\n\n    Returns:\n        list[dict]: List of dictionaries containing vertebrae data.\n    \"\"\"\n    patient_data = []\n    dir_path = DICOM_DATA_DIR / dir_name\n    dicom_reader = DICOMReader(dir_path)\n    tensor, description, metadata = dicom_reader.process_dicom_series()\n    vertebra_extractor = VertebraExtractor(IS_FULL_RESOLUTION, DEVICE)\n\n    patient_data.extend(\n        self._process_injured_vertebrae(\n            vertebra_extractor, tensor, metadata, dir_name, injuried_vertebrae, target_size\n        )\n    )\n\n    patient_data.extend(\n        self._process_healthy_vertebrae(\n            vertebra_extractor,\n            tensor,\n            metadata,\n            dir_name,\n            injuried_vertebrae,\n            target_size,\n            num_healthy,\n        )\n    )\n\n    return patient_data\n</code></pre>"},{"location":"code/data_preparation/#src.data_utils.dataset_creator.DatasetCreator.save_patient_data","title":"<code>save_patient_data(patient_data)</code>","text":"<p>Save processed vertebra data as tensors and append metadata to a CSV file.</p> <p>Each tensor is saved to <code>TENSOR_DIR</code> using a sequential 5-digit filename (e.g., \"00001.pt\"). Corresponding metadata including vertebra name, injury type, patient ID (II), and file path is appended to the CSV file at <code>LABELS_FILE_PATH</code>. If the CSV does not exist, it is created.</p> <p>This function is designed to support resumable dataset creation \u2014 partial results are not lost in case of interruption.</p> <p>Parameters:</p> Name Type Description Default <code>patient_data</code> <code>list[dict]</code> <p>A list of dictionaries, where each dictionary contains: - \"target_tensor\" (torch.Tensor): The extracted vertebra tensor. - \"vertebra\" (str): Name of the vertebra (e.g., \"L1\"). - \"injury_type\" (str): Injury classification (e.g., \"A1\"). - \"II\" (str): Patient ID.</p> required Source code in <code>src/data_utils/dataset_creator.py</code> <pre><code>def save_patient_data(self, patient_data: list[dict]) -&gt; None:\n    \"\"\"\n    Save processed vertebra data as tensors and append metadata to a CSV file.\n\n    Each tensor is saved to `TENSOR_DIR` using a sequential 5-digit filename (e.g., \"00001.pt\").\n    Corresponding metadata including vertebra name, injury type, patient ID (II), and file path\n    is appended to the CSV file at `LABELS_FILE_PATH`. If the CSV does not exist, it is created.\n\n    This function is designed to support resumable dataset creation \u2014 partial results are not lost\n    in case of interruption.\n\n    Args:\n        patient_data (list[dict]):\n            A list of dictionaries, where each dictionary contains:\n            - \"target_tensor\" (torch.Tensor): The extracted vertebra tensor.\n            - \"vertebra\" (str): Name of the vertebra (e.g., \"L1\").\n            - \"injury_type\" (str): Injury classification (e.g., \"A1\").\n            - \"II\" (str): Patient ID.\n    \"\"\"\n    labels_file_exists = LABELS_FILE_PATH.exists()\n    next_index = self.get_max_index() + 1\n\n    with open(LABELS_FILE_PATH, mode=\"a\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(\n            csvfile, fieldnames=[\"vertebra\", \"injury_type\", \"II\", \"tensor_path\"]\n        )\n\n        if not labels_file_exists:\n            writer.writeheader()\n\n        for entry in patient_data:\n            tensor = entry[\"target_tensor\"]\n            vertebra = entry[\"vertebra\"]\n            injury_type = entry[\"injury_type\"]\n            II = entry[\"II\"]\n\n            tensor_file_path = TENSOR_DIR / f\"{next_index:05d}.pt\"\n\n            TENSOR_DIR.mkdir(parents=True, exist_ok=True)\n\n            torch.save(tensor, tensor_file_path)\n\n            writer.writerow(\n                {\n                    \"vertebra\": vertebra,\n                    \"injury_type\": injury_type,\n                    \"II\": II,\n                    \"tensor_path\": str(f\"{next_index:05d}\"),\n                }\n            )\n            next_index += 1\n</code></pre>"},{"location":"code/inference/","title":"Inference","text":""},{"location":"code/inference/#src.inference.predictor.InjuryDetector","title":"<code>InjuryDetector</code>","text":"<p>A high-level wrapper for vertebrae injury classification from DICOM series.</p> <p>This class handles the full pipeline from loading a DICOM series, extracting vertebrae fragments, batching them, running classification, and returning the top-k predictions along with any vertebrae that could not be extracted.</p> Source code in <code>src/inference/predictor.py</code> <pre><code>class InjuryDetector:\n    \"\"\"\n    A high-level wrapper for vertebrae injury classification from DICOM series.\n\n    This class handles the full pipeline from loading a DICOM series, extracting vertebrae\n    fragments, batching them, running classification, and returning the top-k predictions\n    along with any vertebrae that could not be extracted.\n    \"\"\"\n\n    def __init__(\n        self,\n        cls_type: Literal[\"med3d\", \"monai\", \"base\"],\n        cls_path: Path,\n        num_classes: int = 10,\n        **model_kwargs: dict,\n    ):\n        \"\"\"\n        Initialize the injury detector with a specified classifier and weights.\n\n        Args:\n            cls_type (str): Type of classifier to use (\"med3d\", \"monai\", \"base\").\n            cls_path (Path): Path to the pretrained weights (.pth file).\n            num_classes (int): Number of output classes for classification. Defaults to 9.\n            **model_kwargs (dict): Additional keyword arguments passed to the model factory.\n        \"\"\"\n        self.classifier = create_model(\n            model_type=cls_type, num_classes=num_classes, device=DEVICE, **model_kwargs\n        )\n        self.classifier.load_weights(weights_path=cls_path)\n\n    def predict(\n        self,\n        patient_dicom_path: Path,\n        k: int = 2,\n    ) -&gt; tuple[list[dict], set[str]]:\n        \"\"\"\n        Predict the top-k class labels for each vertebra in a patient's DICOM series.\n\n        This method reads a DICOM directory, extracts individual vertebrae (with neighbors),\n        batches them, performs classification using the loaded model, and returns predictions\n        and missing vertebrae.\n\n        Args:\n            patient_dicom_path (Path): Path to the patient's DICOM folder (relative to DICOM_DATA_DIR).\n            k (int): Number of top class predictions to return per vertebra. Defaults to 2.\n\n        Returns:\n            Tuple[list[dict], set[str]]:\n                - A list of dictionaries for each detected vertebra, each with:\n                    {\n                        \"vertebra\": &lt;vertebra_label&gt;,\n                        \"topk\": [ (class_name, probability), ... ]\n                    }\n                - A set of vertebra labels that could not be extracted from the DICOM data.\n        \"\"\"\n        vertebra_extractor = VertebraExtractor(IS_FULL_RESOLUTION, DEVICE)\n        dicom_reader = DICOMReader(DICOM_DATA_DIR / patient_dicom_path)\n        tensor, description, metadata = dicom_reader.process_dicom_series()\n\n        all_vertebrae = sorted(VERTEBRAE_MAP.keys())\n        vertebra_tensors = []\n        vertebra_names = []\n        unfound_vertebrae = set()\n\n        for vertebra in all_vertebrae:\n            vt = vertebra_extractor.extract_vertebrae_with_neighbors(\n                input_tensor=tensor,\n                metadata=metadata,\n                target_vertebrae=vertebra,\n                target_size=TARGET_TENSOR_SIZE,\n            )\n            if vt is None:\n                unfound_vertebrae.add(vertebra)\n                continue\n            if vt.dim() == 5 and vt.shape[0] == 1:\n                vt = vt.squeeze(0)\n            vertebra_tensors.append(vt)\n            vertebra_names.append(vertebra)\n\n        if not vertebra_tensors:\n            return [], unfound_vertebrae\n\n        batch = torch.stack(vertebra_tensors).to(DEVICE)\n        topk_results = self.classifier.topk_predictions(batch, CLASS_NAMES_FILE_PATH, k)\n\n        patient_predictions = [\n            {\"vertebra\": vertebra, \"topk\": topk}\n            for vertebra, topk in zip(vertebra_names, topk_results)\n        ]\n\n        return patient_predictions, unfound_vertebrae\n</code></pre>"},{"location":"code/inference/#src.inference.predictor.InjuryDetector.__init__","title":"<code>__init__(cls_type, cls_path, num_classes=10, **model_kwargs)</code>","text":"<p>Initialize the injury detector with a specified classifier and weights.</p> <p>Parameters:</p> Name Type Description Default <code>cls_type</code> <code>str</code> <p>Type of classifier to use (\"med3d\", \"monai\", \"base\").</p> required <code>cls_path</code> <code>Path</code> <p>Path to the pretrained weights (.pth file).</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes for classification. Defaults to 9.</p> <code>10</code> <code>**model_kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to the model factory.</p> <code>{}</code> Source code in <code>src/inference/predictor.py</code> <pre><code>def __init__(\n    self,\n    cls_type: Literal[\"med3d\", \"monai\", \"base\"],\n    cls_path: Path,\n    num_classes: int = 10,\n    **model_kwargs: dict,\n):\n    \"\"\"\n    Initialize the injury detector with a specified classifier and weights.\n\n    Args:\n        cls_type (str): Type of classifier to use (\"med3d\", \"monai\", \"base\").\n        cls_path (Path): Path to the pretrained weights (.pth file).\n        num_classes (int): Number of output classes for classification. Defaults to 9.\n        **model_kwargs (dict): Additional keyword arguments passed to the model factory.\n    \"\"\"\n    self.classifier = create_model(\n        model_type=cls_type, num_classes=num_classes, device=DEVICE, **model_kwargs\n    )\n    self.classifier.load_weights(weights_path=cls_path)\n</code></pre>"},{"location":"code/inference/#src.inference.predictor.InjuryDetector.predict","title":"<code>predict(patient_dicom_path, k=2)</code>","text":"<p>Predict the top-k class labels for each vertebra in a patient's DICOM series.</p> <p>This method reads a DICOM directory, extracts individual vertebrae (with neighbors), batches them, performs classification using the loaded model, and returns predictions and missing vertebrae.</p> <p>Parameters:</p> Name Type Description Default <code>patient_dicom_path</code> <code>Path</code> <p>Path to the patient's DICOM folder (relative to DICOM_DATA_DIR).</p> required <code>k</code> <code>int</code> <p>Number of top class predictions to return per vertebra. Defaults to 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>tuple[list[dict], set[str]]</code> <p>Tuple[list[dict], set[str]]: - A list of dictionaries for each detected vertebra, each with:     {         \"vertebra\": ,         \"topk\": [ (class_name, probability), ... ]     } - A set of vertebra labels that could not be extracted from the DICOM data. Source code in <code>src/inference/predictor.py</code> <pre><code>def predict(\n    self,\n    patient_dicom_path: Path,\n    k: int = 2,\n) -&gt; tuple[list[dict], set[str]]:\n    \"\"\"\n    Predict the top-k class labels for each vertebra in a patient's DICOM series.\n\n    This method reads a DICOM directory, extracts individual vertebrae (with neighbors),\n    batches them, performs classification using the loaded model, and returns predictions\n    and missing vertebrae.\n\n    Args:\n        patient_dicom_path (Path): Path to the patient's DICOM folder (relative to DICOM_DATA_DIR).\n        k (int): Number of top class predictions to return per vertebra. Defaults to 2.\n\n    Returns:\n        Tuple[list[dict], set[str]]:\n            - A list of dictionaries for each detected vertebra, each with:\n                {\n                    \"vertebra\": &lt;vertebra_label&gt;,\n                    \"topk\": [ (class_name, probability), ... ]\n                }\n            - A set of vertebra labels that could not be extracted from the DICOM data.\n    \"\"\"\n    vertebra_extractor = VertebraExtractor(IS_FULL_RESOLUTION, DEVICE)\n    dicom_reader = DICOMReader(DICOM_DATA_DIR / patient_dicom_path)\n    tensor, description, metadata = dicom_reader.process_dicom_series()\n\n    all_vertebrae = sorted(VERTEBRAE_MAP.keys())\n    vertebra_tensors = []\n    vertebra_names = []\n    unfound_vertebrae = set()\n\n    for vertebra in all_vertebrae:\n        vt = vertebra_extractor.extract_vertebrae_with_neighbors(\n            input_tensor=tensor,\n            metadata=metadata,\n            target_vertebrae=vertebra,\n            target_size=TARGET_TENSOR_SIZE,\n        )\n        if vt is None:\n            unfound_vertebrae.add(vertebra)\n            continue\n        if vt.dim() == 5 and vt.shape[0] == 1:\n            vt = vt.squeeze(0)\n        vertebra_tensors.append(vt)\n        vertebra_names.append(vertebra)\n\n    if not vertebra_tensors:\n        return [], unfound_vertebrae\n\n    batch = torch.stack(vertebra_tensors).to(DEVICE)\n    topk_results = self.classifier.topk_predictions(batch, CLASS_NAMES_FILE_PATH, k)\n\n    patient_predictions = [\n        {\"vertebra\": vertebra, \"topk\": topk}\n        for vertebra, topk in zip(vertebra_names, topk_results)\n    ]\n\n    return patient_predictions, unfound_vertebrae\n</code></pre>"},{"location":"code/inference/#src.inference.api.predict_from_patient_id","title":"<code>predict_from_patient_id(patient_id=Query(..., description='Subdirectory under DICOM_DATA_DIR'), k=Query(2, description='Number of top-k predictions to return per vertebra'))</code>","text":"<p>Predict vertebral injury classes using a patient's DICOM series stored under DICOM_DATA_DIR.</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>str</code> <p>Folder name inside DICOM_DATA_DIR containing the DICOM series.</p> <code>Query(..., description='Subdirectory under DICOM_DATA_DIR')</code> <code>k</code> <code>int</code> <p>Number of top predictions per vertebra.</p> <code>Query(2, description='Number of top-k predictions to return per vertebra')</code> <p>Returns:</p> Type Description <p>JSON with predictions and list of vertebrae not found.</p> Source code in <code>src/inference/api.py</code> <pre><code>@app.get(\"/predict\", response_model=PredictionResponse)\ndef predict_from_patient_id(\n    patient_id: str = Query(..., description=\"Subdirectory under DICOM_DATA_DIR\"),\n    k: int = Query(2, description=\"Number of top-k predictions to return per vertebra\"),\n):\n    \"\"\"\n    Predict vertebral injury classes using a patient's DICOM series stored under DICOM_DATA_DIR.\n\n    Args:\n        patient_id (str): Folder name inside DICOM_DATA_DIR containing the DICOM series.\n        k (int): Number of top predictions per vertebra.\n\n    Returns:\n        JSON with predictions and list of vertebrae not found.\n    \"\"\"\n    dicom_path = DICOM_DATA_DIR / patient_id\n    if not dicom_path.exists() or not dicom_path.is_dir():\n        return JSONResponse(\n            status_code=400, content={\"error\": f\"Directory '{dicom_path}' does not exist.\"}\n        )\n\n    try:\n        predictions, unfound = detector.predict(patient_dicom_path=dicom_path, k=k)\n        return PredictionResponse(\n            predictions=[\n                TopKPrediction(vertebra=p[\"vertebra\"], topk=p[\"topk\"]) for p in predictions\n            ],\n            unfound_vertebrae=sorted(unfound),\n        )\n    except Exception as e:\n        return JSONResponse(status_code=500, content={\"error\": str(e)})\n</code></pre>"},{"location":"code/inference/#src.inference.app.create_heatmap","title":"<code>create_heatmap(heatmap_df)</code>","text":"<p>Create a heatmap visualization.</p> <p>Parameters:</p> Name Type Description Default <code>heatmap_df</code> <code>DataFrame</code> <p>Heatmap data DataFrame.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>go.Figure: Plotly heatmap figure.</p> Source code in <code>src/inference/app.py</code> <pre><code>def create_heatmap(heatmap_df: pd.DataFrame) -&gt; go.Figure:\n    \"\"\"\n    Create a heatmap visualization.\n\n    Args:\n        heatmap_df (pd.DataFrame): Heatmap data DataFrame.\n\n    Returns:\n        go.Figure: Plotly heatmap figure.\n    \"\"\"\n    healthy_bars = heatmap_df[heatmap_df[\"Class\"] == \"H\"].copy()\n    healthy_bars[\"Value\"] = -1 * healthy_bars[\"Probability\"]\n\n    injury_bars = heatmap_df[heatmap_df[\"Class\"] != \"H\"].copy()\n    injury_bars[\"Value\"] = injury_bars[\"Probability\"]\n\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Bar(\n            x=healthy_bars[\"Value\"],\n            y=healthy_bars[\"Vertebra\"],\n            orientation=\"h\",\n            name=\"Zdrowy\",\n            marker_color=\"green\",\n            hovertemplate=\"Kr\u0119g: %{y}&lt;br&gt;H: %{x:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    fig.add_trace(\n        go.Bar(\n            x=injury_bars[\"Value\"],\n            y=injury_bars[\"Vertebra\"],\n            orientation=\"h\",\n            name=\"Uraz\",\n            marker_color=\"crimson\",\n            hovertemplate=\"Kr\u0119g: %{y}&lt;br&gt;Uraz: %{x:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    fig.update_layout(\n        barmode=\"relative\",\n        xaxis_title=\"Prawdopodobie\u0144stwo\",\n        yaxis=dict(categoryorder=\"array\", categoryarray=VERTEBRA_ORDER[::-1]),\n        height=800,\n        showlegend=False,\n        margin=dict(l=10, r=10, t=30, b=10),\n    )\n\n    return fig\n</code></pre>"},{"location":"code/inference/#src.inference.app.dicom_viewer","title":"<code>dicom_viewer(tensor)</code>","text":"<p>Display a DICOM tensor slice in Streamlit.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor representing the DICOM series.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Displays the selected slice in Streamlit.</p> Source code in <code>src/inference/app.py</code> <pre><code>@st.fragment\ndef dicom_viewer(tensor: torch.Tensor) -&gt; None:\n    \"\"\"\n    Display a DICOM tensor slice in Streamlit.\n\n    Args:\n        tensor (torch.Tensor): The tensor representing the DICOM series.\n\n    Returns:\n        None: Displays the selected slice in Streamlit.\n    \"\"\"\n    orientation = st.selectbox(\n        \"Wybierz o\u015b przekroju\", [\"Poprzeczny (Z)\", \"Koronalny (Y)\", \"Strza\u0142kowy (X)\"]\n    )\n    axis = {\"Poprzeczny (Z)\": 0, \"Koronalny (Y)\": 1, \"Strza\u0142kowy (X)\": 2}[orientation]\n    slice_count = tensor.shape[axis]\n    slice_idx = st.slider(\"Wybierz przekr\u00f3j\", 0, slice_count - 1, slice_count // 2)\n\n    if axis == 0:\n        slice_img = tensor[slice_idx, :, :]\n    elif axis == 1:\n        slice_img = tensor[:, slice_idx, :]\n    else:\n        slice_img = tensor[:, :, slice_idx]\n\n    fig, ax = plt.subplots()\n    ax.imshow(slice_img.numpy(), cmap=\"gray\")\n    ax.axis(\"off\")\n    st.pyplot(fig)\n</code></pre>"},{"location":"code/inference/#src.inference.app.fetch_predictions","title":"<code>fetch_predictions(patient_id, k)</code>","text":"<p>Fetch predictions from the API.</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>str</code> <p>Selected patient ID.</p> required <code>k</code> <code>int</code> <p>Number of top-k classes.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>JSON response from the API.</p> Source code in <code>src/inference/app.py</code> <pre><code>def fetch_predictions(patient_id: str, k: int) -&gt; Dict:\n    \"\"\"\n    Fetch predictions from the API.\n\n    Args:\n        patient_id (str): Selected patient ID.\n        k (int): Number of top-k classes.\n\n    Returns:\n        Dict: JSON response from the API.\n    \"\"\"\n    params = {\"patient_id\": patient_id, \"k\": k}\n    response = requests.get(API_URL, params=params)\n    return response.json()\n</code></pre>"},{"location":"code/inference/#src.inference.app.get_dicom_dirs","title":"<code>get_dicom_dirs()</code>","text":"<p>Get a list of available DICOM directories.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Sorted list of directory names.</p> Source code in <code>src/inference/app.py</code> <pre><code>def get_dicom_dirs() -&gt; List[str]:\n    \"\"\"\n    Get a list of available DICOM directories.\n\n    Returns:\n        List[str]: Sorted list of directory names.\n    \"\"\"\n    return sorted([p.name for p in DICOM_DATA_DIR.iterdir() if p.is_dir()])\n</code></pre>"},{"location":"code/inference/#src.inference.app.load_patient_tensor","title":"<code>load_patient_tensor(patient_path)</code>","text":"<p>Load the tensor for a given patient from DICOM files.</p> <p>Parameters:</p> Name Type Description Default <code>patient_path</code> <code>Path</code> <p>Path to the patient's DICOM directory.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, str, dict]</code> <p>tuple[torch.Tensor, str, dict]: A tuple containing: - The tensor representing the DICOM series. - A description of the series. - Metadata associated with the series.</p> Source code in <code>src/inference/app.py</code> <pre><code>@st.cache_resource\ndef load_patient_tensor(patient_path: Path) -&gt; tuple[torch.Tensor, str, dict]:\n    \"\"\"\n    Load the tensor for a given patient from DICOM files.\n\n    Args:\n        patient_path (Path): Path to the patient's DICOM directory.\n\n    Returns:\n        tuple[torch.Tensor, str, dict]: A tuple containing:\n            - The tensor representing the DICOM series.\n            - A description of the series.\n            - Metadata associated with the series.\n    \"\"\"\n    reader = DICOMReader(patient_path)\n    return reader.process_dicom_series()\n</code></pre>"},{"location":"code/inference/#src.inference.app.main","title":"<code>main()</code>","text":"<p>Main function to run the Streamlit app.</p> Source code in <code>src/inference/app.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to run the Streamlit app.\n    \"\"\"\n    st.set_page_config(layout=\"wide\")\n\n    dicom_dirs = get_dicom_dirs()\n    selected_patient = st.sidebar.selectbox(\"Pacjent (folder)\", dicom_dirs)\n    k = st.sidebar.slider(\"Liczba top-k klas\", min_value=1, max_value=5, value=2)\n\n    data = fetch_predictions(selected_patient, k)\n    df = process_predictions(data, k)\n    heatmap_df = process_heatmap_data(data)\n    tensor, _, _ = load_patient_tensor(DICOM_DATA_DIR / selected_patient)\n\n    styled_df = style_predictions(df)\n\n    st.title(f\"Wyniki klasyfikacji \u2013 {selected_patient}\")\n    col1, col2, col3 = st.columns([2, 1, 2], gap=\"large\")\n\n    with col1:\n        st.subheader(\"Tabela predykcji\")\n        st.dataframe(styled_df, use_container_width=True, height=800)\n\n    with col2:\n        st.subheader(\"Pewno\u015b\u0107 predykcji\")\n        fig = create_heatmap(heatmap_df)\n        st.plotly_chart(fig, use_container_width=True)\n    with col3:\n        st.subheader(\"Podgl\u0105d DICOM\")\n        if tensor is not None:\n            dicom_viewer(tensor)\n        else:\n            st.error(\"Nie znaleziono poprawnego DICOMu.\")\n\n    unfound = data.get(\"unfound_vertebrae\", [])\n    if unfound:\n        st.warning(\"Nie wykryto kr\u0119g\u00f3w: \" + \", \".join(sorted(unfound)))\n</code></pre>"},{"location":"code/inference/#src.inference.app.process_heatmap_data","title":"<code>process_heatmap_data(data)</code>","text":"<p>Process data for heatmap visualization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>API response data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Heatmap data DataFrame.</p> Source code in <code>src/inference/app.py</code> <pre><code>def process_heatmap_data(data: Dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Process data for heatmap visualization.\n\n    Args:\n        data (Dict): API response data.\n\n    Returns:\n        pd.DataFrame: Heatmap data DataFrame.\n    \"\"\"\n    heatmap_data = []\n    for vertebra in VERTEBRA_ORDER:\n        preds = next(\n            (entry[\"topk\"] for entry in data[\"predictions\"] if entry[\"vertebra\"] == vertebra), None\n        )\n        if preds:\n            cls0, prob0 = preds[0]\n            heatmap_data.append({\"Vertebra\": vertebra, \"Class\": cls0, \"Probability\": prob0})\n        else:\n            heatmap_data.append({\"Vertebra\": vertebra, \"Class\": \"\", \"Probability\": 0.0})\n    return pd.DataFrame(heatmap_data)\n</code></pre>"},{"location":"code/inference/#src.inference.app.process_predictions","title":"<code>process_predictions(data, k)</code>","text":"<p>Process predictions into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict</code> <p>API response data.</p> required <code>k</code> <code>int</code> <p>Number of top-k classes.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed predictions DataFrame.</p> Source code in <code>src/inference/app.py</code> <pre><code>def process_predictions(data: Dict, k: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Process predictions into a DataFrame.\n\n    Args:\n        data (Dict): API response data.\n        k (int): Number of top-k classes.\n\n    Returns:\n        pd.DataFrame: Processed predictions DataFrame.\n    \"\"\"\n    rows = []\n    for vertebra in VERTEBRA_ORDER:\n        row = {\"Vertebra\": vertebra}\n        preds = next(\n            (entry[\"topk\"] for entry in data[\"predictions\"] if entry[\"vertebra\"] == vertebra), None\n        )\n        if preds:\n            for i in range(k):\n                if i &lt; len(preds):\n                    cls, prob = preds[i]\n                    row[f\"Top-{i+1} Class\"] = cls\n                    row[f\"Top-{i+1} Prob\"] = float(prob)\n        else:\n            for i in range(k):\n                row[f\"Top-{i+1} Class\"] = \"\"\n                row[f\"Top-{i+1} Prob\"] = None\n        rows.append(row)\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"code/inference/#src.inference.app.style_predictions","title":"<code>style_predictions(df)</code>","text":"<p>Apply styling to the predictions DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Predictions DataFrame.</p> required Source code in <code>src/inference/app.py</code> <pre><code>def style_predictions(df: pd.DataFrame):\n    \"\"\"\n    Apply styling to the predictions DataFrame.\n\n    Args:\n        df (pd.DataFrame): Predictions DataFrame.\n    \"\"\"\n\n    def color_row(row):\n        cls = row.get(\"Top-1 Class\")\n        prob = row.get(\"Top-1 Prob\")\n        if pd.isna(prob) or cls is None:\n            return [\"\"] * len(row)\n        prob = float(prob)\n        if cls == \"H\":\n            return [f\"background-color: rgba(0,255,0,{0.3 * prob}); color: black\"] * len(row)\n        return [f\"background-color: rgba(255,0,0,{0.3 * prob}); color: white\"] * len(row)\n\n    return df.style.apply(color_row, axis=1)\n</code></pre>"},{"location":"code/modeling/","title":"Modeling","text":""},{"location":"code/modeling/#src.modeling.model_factory.create_model","title":"<code>create_model(model_type, num_classes, device=torch.device('cuda'), **kwargs)</code>","text":"<p>Factory function that returns a ready-to-use vertebrae classification model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of model to create (\"med3d\", \"monai\", \"resnet\").</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>device</code> <code>device</code> <p>Target device (default: \"cuda\").</p> <code>device('cuda')</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters for model configuration.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VertebraeClassifier</code> <code>VertebraeClassifier</code> <p>Ready model in eval mode on given device.</p> Source code in <code>src/modeling/model_factory.py</code> <pre><code>def create_model(\n    model_type: Literal[\"med3d\", \"monai\", \"base\"],\n    num_classes: int,\n    device: torch.device = torch.device(\"cuda\"),\n    **kwargs: dict,\n) -&gt; VertebraeClassifier:\n    \"\"\"\n    Factory function that returns a ready-to-use vertebrae classification model.\n\n    Args:\n        model_type (str): Type of model to create (\"med3d\", \"monai\", \"resnet\").\n        num_classes (int): Number of output classes.\n        device (torch.device): Target device (default: \"cuda\").\n        **kwargs (dict): Additional parameters for model configuration.\n\n    Returns:\n        VertebraeClassifier: Ready model in eval mode on given device.\n    \"\"\"\n    if model_type == \"med3d\":\n        model = Med3DClassifier(\n            num_classes=num_classes,\n            device=device,\n            load_pretrained=True,\n            **kwargs,\n        )\n    elif model_type == \"monai\":\n        model = SegResNetClassifier(num_classes=num_classes)\n    elif model_type == \"base\":\n        model = Med3DClassifier(\n            num_classes=num_classes,\n            device=device,\n            load_pretrained=False,\n            **kwargs,\n        )\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n\n    model = model.to(device)\n    model.eval()\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier","title":"<code>VertebraeClassifier</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/modeling/base_model.py</code> <pre><code>class VertebraeClassifier(nn.Module, metaclass=abc.ABCMeta):\n    def __init__(self, num_classes: int) -&gt; None:\n        \"\"\"\n        VertebraeClassifier is an abstract base class for vertebrae classification models.\n        Args:\n            num_classes (int): Number of classes for classification.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n\n    @abc.abstractmethod\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        pass\n\n    def predict(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Predict the class of the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Predicted class indices.\n        \"\"\"\n        logits = self.forward(x)\n        return torch.argmax(torch.softmax(logits, dim=1), dim=1)\n\n    def topk_predictions(\n        self, x: torch.Tensor, class_names_path: Path, k: int = 3\n    ) -&gt; list[list[tuple[str, float]]]:\n        \"\"\"\n        Return the top-k predicted classes and their probabilities for each input sample.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (N, ...).\n            k (int): Number of top predictions to return. Defaults to 3.\n            class_names_path (Path): Path to a text file containing class names (one per line).\n\n        Returns:\n            List[List[Tuple[str, float]]]: For each input sample, a list of (class_name, probability) tuples.\n        \"\"\"\n        with open(class_names_path, \"r\") as f:\n            class_names = [line.strip() for line in f if line.strip()]\n\n        if len(class_names) != self.num_classes:\n            raise ValueError(\n                f\"Number of class names ({len(class_names)}) does not match model output ({self.num_classes}).\"\n            )\n\n        self.eval()\n        with torch.inference_mode():\n            logits = self.forward(x)\n            probs = torch.softmax(logits, dim=1)\n            top_probs, top_idxs = torch.topk(probs, k=k, dim=1)\n\n        results = []\n        for sample_probs, sample_idxs in zip(top_probs, top_idxs):\n            sample_result = [\n                (class_names[idx], float(prob)) for idx, prob in zip(sample_idxs, sample_probs)\n            ]\n            results.append(sample_result)\n\n        return results\n\n    def compute_loss(\n        self, x: torch.Tensor, labels: torch.Tensor, criterion: nn.Module\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the loss for the given input and labels.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            labels (torch.Tensor): Ground truth labels.\n            criterion (nn.Module): Loss function.\n\n        Returns:\n            torch.Tensor: Computed loss.\n        \"\"\"\n        logits = self.forward(x)\n        return criterion(logits, labels)\n\n    def load_weights(self, weights_path: Path, strict: bool = True, assign: bool = False):\n        \"\"\"\n        Load pretrained weights into the model from a .pth file.\n\n        Args:\n            weights_path (Path): Path to the .pth file containing the saved state_dict.\n            strict (bool, optional): Whether to strictly enforce that the keys in the state_dict match\n                the model's keys. If False, unmatched keys will be ignored. Defaults to True.\n            assign (bool, optional): If True, assigns weights without in-place modification\n                (requires PyTorch &gt;= 2.1). Defaults to False.\n\n        Raises:\n            RuntimeError: If loading fails due to mismatched keys and `strict=True`.\n            TypeError: If the loaded file is not a valid state_dict.\n        \"\"\"\n        self.load_state_dict(torch.load(weights_path), strict, assign)\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier.__init__","title":"<code>__init__(num_classes)</code>","text":"<p>VertebraeClassifier is an abstract base class for vertebrae classification models. Args:     num_classes (int): Number of classes for classification.</p> Source code in <code>src/modeling/base_model.py</code> <pre><code>def __init__(self, num_classes: int) -&gt; None:\n    \"\"\"\n    VertebraeClassifier is an abstract base class for vertebrae classification models.\n    Args:\n        num_classes (int): Number of classes for classification.\n    \"\"\"\n    super().__init__()\n    self.num_classes = num_classes\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier.compute_loss","title":"<code>compute_loss(x, labels, criterion)</code>","text":"<p>Compute the loss for the given input and labels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>labels</code> <code>Tensor</code> <p>Ground truth labels.</p> required <code>criterion</code> <code>Module</code> <p>Loss function.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Computed loss.</p> Source code in <code>src/modeling/base_model.py</code> <pre><code>def compute_loss(\n    self, x: torch.Tensor, labels: torch.Tensor, criterion: nn.Module\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the loss for the given input and labels.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n        labels (torch.Tensor): Ground truth labels.\n        criterion (nn.Module): Loss function.\n\n    Returns:\n        torch.Tensor: Computed loss.\n    \"\"\"\n    logits = self.forward(x)\n    return criterion(logits, labels)\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor.</p> Source code in <code>src/modeling/base_model.py</code> <pre><code>@abc.abstractmethod\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier.load_weights","title":"<code>load_weights(weights_path, strict=True, assign=False)</code>","text":"<p>Load pretrained weights into the model from a .pth file.</p> <p>Parameters:</p> Name Type Description Default <code>weights_path</code> <code>Path</code> <p>Path to the .pth file containing the saved state_dict.</p> required <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys in the state_dict match the model's keys. If False, unmatched keys will be ignored. Defaults to True.</p> <code>True</code> <code>assign</code> <code>bool</code> <p>If True, assigns weights without in-place modification (requires PyTorch &gt;= 2.1). Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If loading fails due to mismatched keys and <code>strict=True</code>.</p> <code>TypeError</code> <p>If the loaded file is not a valid state_dict.</p> Source code in <code>src/modeling/base_model.py</code> <pre><code>def load_weights(self, weights_path: Path, strict: bool = True, assign: bool = False):\n    \"\"\"\n    Load pretrained weights into the model from a .pth file.\n\n    Args:\n        weights_path (Path): Path to the .pth file containing the saved state_dict.\n        strict (bool, optional): Whether to strictly enforce that the keys in the state_dict match\n            the model's keys. If False, unmatched keys will be ignored. Defaults to True.\n        assign (bool, optional): If True, assigns weights without in-place modification\n            (requires PyTorch &gt;= 2.1). Defaults to False.\n\n    Raises:\n        RuntimeError: If loading fails due to mismatched keys and `strict=True`.\n        TypeError: If the loaded file is not a valid state_dict.\n    \"\"\"\n    self.load_state_dict(torch.load(weights_path), strict, assign)\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier.predict","title":"<code>predict(x)</code>","text":"<p>Predict the class of the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Predicted class indices.</p> Source code in <code>src/modeling/base_model.py</code> <pre><code>def predict(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Predict the class of the input tensor.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Predicted class indices.\n    \"\"\"\n    logits = self.forward(x)\n    return torch.argmax(torch.softmax(logits, dim=1), dim=1)\n</code></pre>"},{"location":"code/modeling/#src.modeling.base_model.VertebraeClassifier.topk_predictions","title":"<code>topk_predictions(x, class_names_path, k=3)</code>","text":"<p>Return the top-k predicted classes and their probabilities for each input sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (N, ...).</p> required <code>k</code> <code>int</code> <p>Number of top predictions to return. Defaults to 3.</p> <code>3</code> <code>class_names_path</code> <code>Path</code> <p>Path to a text file containing class names (one per line).</p> required <p>Returns:</p> Type Description <code>list[list[tuple[str, float]]]</code> <p>List[List[Tuple[str, float]]]: For each input sample, a list of (class_name, probability) tuples.</p> Source code in <code>src/modeling/base_model.py</code> <pre><code>def topk_predictions(\n    self, x: torch.Tensor, class_names_path: Path, k: int = 3\n) -&gt; list[list[tuple[str, float]]]:\n    \"\"\"\n    Return the top-k predicted classes and their probabilities for each input sample.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (N, ...).\n        k (int): Number of top predictions to return. Defaults to 3.\n        class_names_path (Path): Path to a text file containing class names (one per line).\n\n    Returns:\n        List[List[Tuple[str, float]]]: For each input sample, a list of (class_name, probability) tuples.\n    \"\"\"\n    with open(class_names_path, \"r\") as f:\n        class_names = [line.strip() for line in f if line.strip()]\n\n    if len(class_names) != self.num_classes:\n        raise ValueError(\n            f\"Number of class names ({len(class_names)}) does not match model output ({self.num_classes}).\"\n        )\n\n    self.eval()\n    with torch.inference_mode():\n        logits = self.forward(x)\n        probs = torch.softmax(logits, dim=1)\n        top_probs, top_idxs = torch.topk(probs, k=k, dim=1)\n\n    results = []\n    for sample_probs, sample_idxs in zip(top_probs, top_idxs):\n        sample_result = [\n            (class_names[idx], float(prob)) for idx, prob in zip(sample_idxs, sample_probs)\n        ]\n        results.append(sample_result)\n\n    return results\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.conv3x3x3","title":"<code>conv3x3x3(in_planes, out_planes, stride=1, dilation=1)</code>","text":"<p>3x3x3 convolution with padding</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def conv3x3x3(in_planes: int, out_planes: int, stride: int = 1, dilation: int = 1) -&gt; nn.Conv3d:\n    \"\"\"3x3x3 convolution with padding\"\"\"\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        dilation=dilation,\n        stride=stride,\n        padding=dilation,\n        bias=False,\n    )\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.get_resnet","title":"<code>get_resnet(model_depth=18, **kwargs)</code>","text":"<p>Get a ResNet model based on the specified depth.</p> <p>Parameters:</p> Name Type Description Default <code>model_depth</code> <code>int</code> <p>Depth of the ResNet model (e.g., 10, 18, 34, 50, 101, 152, 200).</p> <code>18</code> <code>**kwargs</code> <p>Additional arguments for the ResNet constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <p>nn.Module: The ResNet model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def get_resnet(model_depth: int = 18, **kwargs):\n    \"\"\"\n    Get a ResNet model based on the specified depth.\n\n    Args:\n        model_depth (int): Depth of the ResNet model (e.g., 10, 18, 34, 50, 101, 152, 200).\n        **kwargs: Additional arguments for the ResNet constructor.\n\n    Returns:\n        nn.Module: The ResNet model.\n    \"\"\"\n    if model_depth == 10:\n        return resnet10(**kwargs)\n    elif model_depth == 18:\n        return resnet18(**kwargs)\n    elif model_depth == 34:\n        return resnet34(**kwargs)\n    elif model_depth == 50:\n        return resnet50(**kwargs)\n    elif model_depth == 101:\n        return resnet101(**kwargs)\n    elif model_depth == 152:\n        return resnet152(**kwargs)\n    elif model_depth == 200:\n        return resnet200(**kwargs)\n    else:\n        raise ValueError(f\"Unsupported ResNet depth: {model_depth}\")\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet10","title":"<code>resnet10(**kwargs)</code>","text":"<p>Constructs a ResNet-18 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet10(**kwargs):\n    \"\"\"Constructs a ResNet-18 model.\"\"\"\n    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet101","title":"<code>resnet101(**kwargs)</code>","text":"<p>Constructs a ResNet-101 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet101(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet152","title":"<code>resnet152(**kwargs)</code>","text":"<p>Constructs a ResNet-101 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet152(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet18","title":"<code>resnet18(**kwargs)</code>","text":"<p>Constructs a ResNet-18 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet18(**kwargs):\n    \"\"\"Constructs a ResNet-18 model.\"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet200","title":"<code>resnet200(**kwargs)</code>","text":"<p>Constructs a ResNet-101 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet200(**kwargs):\n    \"\"\"Constructs a ResNet-101 model.\"\"\"\n    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet34","title":"<code>resnet34(**kwargs)</code>","text":"<p>Constructs a ResNet-34 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet34(**kwargs):\n    \"\"\"Constructs a ResNet-34 model.\"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.resnet3d.resnet50","title":"<code>resnet50(**kwargs)</code>","text":"<p>Constructs a ResNet-50 model.</p> Source code in <code>src/modeling/resnet3d.py</code> <pre><code>def resnet50(**kwargs):\n    \"\"\"Constructs a ResNet-50 model.\"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    return model\n</code></pre>"},{"location":"code/modeling/#src.modeling.med3_transfer.Med3DClassifier","title":"<code>Med3DClassifier</code>","text":"<p>               Bases: <code>VertebraeClassifier</code></p> Source code in <code>src/modeling/med3_transfer.py</code> <pre><code>class Med3DClassifier(VertebraeClassifier):\n    def __init__(\n        self,\n        num_classes: int,\n        model_depth: int = 18,\n        shortcut_type: Literal[\"A\", \"B\"] = \"B\",\n        load_pretrained: bool = True,\n        freeze_backbone: bool = True,\n        device: torch.device = torch.device(\"cuda\"),\n    ) -&gt; None:\n        \"\"\"\n        Classifier using Med3D ResNet backbone with optional pretrained weights.\n\n        Args:\n            num_classes (int): Number of output classes.\n            model_depth (int): Depth of ResNet (e.g., 10, 18, 34...).\n            shortcut_type (Literal['A', 'B']): Type of shortcut connection.\n            load_pretrained (bool): Whether to load pretrained weights from Med3D.\n            freeze_backbone (bool): Whether to freeze the backbone weights.\n        \"\"\"\n        super().__init__(num_classes)\n        self.device = device\n\n        self.model = get_resnet(\n            model_depth=model_depth,\n            num_classes=num_classes,\n            shortcut_type=shortcut_type,\n        )\n\n        if load_pretrained:\n            self._load_med3d_weights(model_depth)\n\n        if freeze_backbone:\n            self._freeze_backbone_weights()\n\n    def _load_med3d_weights(self, model_depth: int) -&gt; None:\n        \"\"\"\n        Load pretrained Med3D weights from HuggingFace, skipping the classification layer.\n        \"\"\"\n        hf_mapping = {\n            10: (\"TencentMedicalNet/MedicalNet-ResNet10\", \"resnet_10.pth\"),\n            18: (\"TencentMedicalNet/MedicalNet-ResNet18\", \"resnet_18.pth\"),\n            34: (\"TencentMedicalNet/MedicalNet-ResNet34\", \"resnet_34.pth\"),\n            50: (\"TencentMedicalNet/MedicalNet-ResNet50\", \"resnet_50.pth\"),\n            101: (\"TencentMedicalNet/MedicalNet-ResNet101\", \"resnet_101.pth\"),\n            152: (\"TencentMedicalNet/MedicalNet-ResNet152\", \"resnet_152.pth\"),\n            200: (\"TencentMedicalNet/MedicalNet-ResNet200\", \"resnet_200.pth\"),\n        }\n\n        if model_depth not in hf_mapping:\n            raise ValueError(f\"No pretrained weights available for model depth {model_depth}\")\n\n        repo_id, filename = hf_mapping[model_depth]\n        weight_path = hf_hub_download(\n            repo_id=repo_id, filename=filename, cache_dir=MODELS_DIR / \"med3d\"\n        )\n\n        state_dict = torch.load(weight_path, map_location=self.device)\n\n        state_dict = {\n            k.replace(\"module.\", \"\"): v\n            for k, v in state_dict.items()\n            if not k.startswith(\"conv-seg\")\n        }\n\n        missing, unexpected = self.model.load_state_dict(state_dict, strict=False)\n        print(\n            f\"[Med3D] Loaded weights with {len(missing)} missing and {len(unexpected)} unexpected keys.\"\n        )\n\n    def _freeze_backbone_weights(self) -&gt; None:\n        \"\"\"\n        Freeze the backbone weights to prevent them from being updated during training.\n        \"\"\"\n        for param in self.model.parameters():\n            param.requires_grad = False\n        for param in self.model.fc.parameters():\n            param.requires_grad = True\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.model(x)\n</code></pre>"},{"location":"code/modeling/#src.modeling.med3_transfer.Med3DClassifier.__init__","title":"<code>__init__(num_classes, model_depth=18, shortcut_type='B', load_pretrained=True, freeze_backbone=True, device=torch.device('cuda'))</code>","text":"<p>Classifier using Med3D ResNet backbone with optional pretrained weights.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>model_depth</code> <code>int</code> <p>Depth of ResNet (e.g., 10, 18, 34...).</p> <code>18</code> <code>shortcut_type</code> <code>Literal['A', 'B']</code> <p>Type of shortcut connection.</p> <code>'B'</code> <code>load_pretrained</code> <code>bool</code> <p>Whether to load pretrained weights from Med3D.</p> <code>True</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone weights.</p> <code>True</code> Source code in <code>src/modeling/med3_transfer.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int,\n    model_depth: int = 18,\n    shortcut_type: Literal[\"A\", \"B\"] = \"B\",\n    load_pretrained: bool = True,\n    freeze_backbone: bool = True,\n    device: torch.device = torch.device(\"cuda\"),\n) -&gt; None:\n    \"\"\"\n    Classifier using Med3D ResNet backbone with optional pretrained weights.\n\n    Args:\n        num_classes (int): Number of output classes.\n        model_depth (int): Depth of ResNet (e.g., 10, 18, 34...).\n        shortcut_type (Literal['A', 'B']): Type of shortcut connection.\n        load_pretrained (bool): Whether to load pretrained weights from Med3D.\n        freeze_backbone (bool): Whether to freeze the backbone weights.\n    \"\"\"\n    super().__init__(num_classes)\n    self.device = device\n\n    self.model = get_resnet(\n        model_depth=model_depth,\n        num_classes=num_classes,\n        shortcut_type=shortcut_type,\n    )\n\n    if load_pretrained:\n        self._load_med3d_weights(model_depth)\n\n    if freeze_backbone:\n        self._freeze_backbone_weights()\n</code></pre>"},{"location":"code/modeling/#src.modeling.monai_resnet.SegResNetClassifier","title":"<code>SegResNetClassifier</code>","text":"<p>               Bases: <code>VertebraeClassifier</code></p> Source code in <code>src/modeling/monai_resnet.py</code> <pre><code>class SegResNetClassifier(VertebraeClassifier):\n    def __init__(self, num_classes: int = 10) -&gt; None:\n        \"\"\"\n        Classifier using MONAI's SegResNet backbone.\n\n        Args:\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super().__init__(num_classes)\n        self._load_monai_model()\n        self._delete_decoder_layers()\n        self._add_classifier_layers()\n\n    def _load_monai_model(self) -&gt; None:\n        \"\"\"\n        Load the MONAI SegResNet model from the specified directory.\n        \"\"\"\n        config_path = SEG_MODEL_DIR / \"configs/inference.json\"\n        parser = ConfigParser()\n        parser.read_config(config_path)\n\n        self.model = parser.get_parsed_content(\"network_def\")\n        weights = torch.load(SEG_MODEL_DIR / \"models/model.pt\", map_location=\"cpu\")\n        if isinstance(weights, dict) and \"state_dict\" in weights:\n            weights = weights[\"state_dict\"]\n        self.model.load_state_dict(weights)\n\n    def _delete_decoder_layers(self) -&gt; None:\n        \"\"\"\n        Remove decoder layers from the model to keep only the encoder part.\n        \"\"\"\n        for attr in [\"up_samples\", \"up_conv\", \"final_conv\"]:\n            if hasattr(self.model, attr):\n                delattr(self.model, attr)\n\n    def _add_classifier_layers(self) -&gt; None:\n        \"\"\"\n        Add classifier layers to the model.\n        \"\"\"\n        self.model.pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.model.flatten = nn.Flatten()\n        self.model.classifier = nn.Linear(256, self.num_classes)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        x = self.model.convInit(x)\n        for down in self.model.down_layers:\n            x = down(x)\n        x = self.model.pool(x)\n        x = self.model.flatten(x)\n        x = self.model.classifier(x)\n        return x\n</code></pre>"},{"location":"code/modeling/#src.modeling.monai_resnet.SegResNetClassifier.__init__","title":"<code>__init__(num_classes=10)</code>","text":"<p>Classifier using MONAI's SegResNet backbone.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>10</code> Source code in <code>src/modeling/monai_resnet.py</code> <pre><code>def __init__(self, num_classes: int = 10) -&gt; None:\n    \"\"\"\n    Classifier using MONAI's SegResNet backbone.\n\n    Args:\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super().__init__(num_classes)\n    self._load_monai_model()\n    self._delete_decoder_layers()\n    self._add_classifier_layers()\n</code></pre>"},{"location":"code/modeling/#src.modeling.monai_resnet.SegResNetClassifier.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor.</p> Source code in <code>src/modeling/monai_resnet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    x = self.model.convInit(x)\n    for down in self.model.down_layers:\n        x = down(x)\n    x = self.model.pool(x)\n    x = self.model.flatten(x)\n    x = self.model.classifier(x)\n    return x\n</code></pre>"},{"location":"code/training/","title":"Modeling","text":""},{"location":"code/training/#src.training.engine.Trainer","title":"<code>Trainer</code>","text":"<p>Trainer class for supervised classification of 3D data.</p> <p>Features: - Logs metrics to Weights &amp; Biases (optional). - Supports learning rate scheduler. - Saves best model checkpoint locally. - Logs final confusion matrix for best model only.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model.</p> required <code>train_loader</code> <code>DataLoader</code> <p>Training data loader.</p> required <code>val_loader</code> <code>DataLoader</code> <p>Validation data loader.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer for training.</p> required <code>criterion</code> <code>Module</code> <p>Loss function.</p> required <code>device</code> <code>device</code> <p>Target device (CPU/GPU).</p> required <code>run_name</code> <code>str</code> <p>Name for saving best model and wandb run.</p> required <code>scheduler</code> <code>Optional[_LRScheduler]</code> <p>Optional learning rate scheduler.</p> <code>None</code> <code>save_dir</code> <code>Path</code> <p>Directory for saving checkpoints.</p> <code>MODELS_DIR / 'cls'</code> <code>log_wandb</code> <code>bool</code> <p>Whether to log metrics to Weights &amp; Biases.</p> <code>True</code> <code>class_names</code> <code>Optional[list[str]]</code> <p>Class labels for confusion matrix.</p> <code>None</code> <code>max_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>50</code> Source code in <code>src/training/engine.py</code> <pre><code>class Trainer:\n    \"\"\"\n    Trainer class for supervised classification of 3D data.\n\n    Features:\n    - Logs metrics to Weights &amp; Biases (optional).\n    - Supports learning rate scheduler.\n    - Saves best model checkpoint locally.\n    - Logs final confusion matrix for best model only.\n\n    Args:\n        model (nn.Module): PyTorch model.\n        train_loader (DataLoader): Training data loader.\n        val_loader (DataLoader): Validation data loader.\n        optimizer (Optimizer): Optimizer for training.\n        criterion (nn.Module): Loss function.\n        device (torch.device): Target device (CPU/GPU).\n        run_name (str): Name for saving best model and wandb run.\n        scheduler (Optional[_LRScheduler]): Optional learning rate scheduler.\n        save_dir (Path): Directory for saving checkpoints.\n        log_wandb (bool): Whether to log metrics to Weights &amp; Biases.\n        class_names (Optional[list[str]]): Class labels for confusion matrix.\n        max_epochs (int): Number of training epochs.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        train_loader: torch.utils.data.DataLoader,\n        val_loader: torch.utils.data.DataLoader,\n        optimizer: torch.optim.Optimizer,\n        criterion: nn.CrossEntropyLoss,\n        device: torch.device,\n        run_name: str,\n        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n        early_stopping: bool = True,\n        early_stopping_patience: int = 10,\n        save_dir: Path = MODELS_DIR / \"cls\",\n        log_wandb: bool = True,\n        class_names: Optional[list[str]] = None,\n        max_epochs: int = 50,\n    ) -&gt; None:\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.early_stopping = early_stopping\n        self.early_stopping_patience = early_stopping_patience\n        self.criterion = criterion\n        self.device = device\n        self.save_dir = save_dir\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        self.log_wandb = log_wandb\n        self.max_epochs = max_epochs\n        self.run_name = run_name\n        self.class_names = class_names\n        num_classes = len(self.class_names) if class_names is not None else 9\n\n        average = \"macro\"\n        self.train_metrics: dict[str, MulticlassStatScores] = {\n            \"acc\": MulticlassAccuracy(num_classes=num_classes).to(device),\n            \"balanced_acc\": MulticlassAccuracy(num_classes=num_classes, average=average),\n            \"mcc\": MulticlassMatthewsCorrCoef(num_classes=num_classes).to(device),\n            \"kappa\": MulticlassCohenKappa(num_classes=num_classes).to(device),\n            \"jaccard\": MulticlassJaccardIndex(num_classes=num_classes, average=average).to(device),\n            \"precision\": MulticlassPrecision(num_classes=num_classes, average=average).to(device),\n            \"recall\": MulticlassRecall(num_classes=num_classes, average=average).to(device),\n            \"f1\": MulticlassF1Score(num_classes=num_classes, average=average).to(device),\n        }\n        self.val_metrics: dict[str, MulticlassStatScores] = {\n            k: copy.deepcopy(v).to(device) for k, v in self.train_metrics.items()\n        }\n\n        self.best_val_loss = float(\"inf\")\n        self.best_model_path = self.save_dir / f\"{run_name}_best.pt\"\n        self._val_preds: list[int] = []\n        self._val_targets: list[int] = []\n\n        if self.log_wandb:\n            wandb.watch(self.model)\n\n    def train(self):\n        \"\"\"\n        Main training loop.\n\n        For each epoch:\n        - Runs training and validation\n        - Logs metrics to wandb (if enabled)\n        - Saves best model (lowest validation loss)\n        - Stores predictions and labels to log confusion matrix later\n        \"\"\"\n        epochs_no_improve = 0\n        for epoch in range(1, self.max_epochs + 1):\n            train_loss, train_metrics = self._train_one_epoch()\n            val_loss, val_preds, val_targets, val_metrics = self._validate()\n\n            if self.scheduler:\n                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                    self.scheduler.step(val_loss)\n                else:\n                    self.scheduler.step()\n\n            if self.log_wandb:\n                wandb.log(\n                    {\n                        \"epoch\": epoch,\n                        \"train_loss\": train_loss,\n                        \"val_loss\": val_loss,\n                        \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n                        **train_metrics,\n                        **val_metrics,\n                    }\n                )\n\n            if val_loss &lt; self.best_val_loss:\n                self.best_val_loss = val_loss\n                torch.save(self.model.state_dict(), self.best_model_path)\n                self._val_preds = val_preds\n                self._val_targets = val_targets\n                epochs_no_improve = 0\n            else:\n                epochs_no_improve += 1\n\n            if self.early_stopping and epochs_no_improve &gt;= self.early_stopping_patience:\n                print(\n                    f\"\u23f9\ufe0f Early stopping at epoch {epoch} (no improvement for {self.early_stopping_patience} epochs)\"\n                )\n                break\n\n        if self.log_wandb and self._val_preds and self._val_targets:\n            if not self.class_names:\n                self.class_names = [str(i) for i in sorted(set(self._val_targets))]\n\n            wandb.log(\n                {\n                    \"val_confusion_matrix\": wandb.plot.confusion_matrix(\n                        y_true=self._val_targets,\n                        preds=self._val_preds,\n                        class_names=self.class_names,\n                    )\n                }\n            )\n\n        print(f\"\u2705 Best model saved at: {self.best_model_path}\")\n\n    def _train_one_epoch(self) -&gt; tuple[float, dict[str, torch.Tensor]]:\n        \"\"\"\n        Runs one epoch of training.\n\n        Returns:\n            avg_loss (float): Average training loss.\n            metrics_out (dict[str, torch.Tensor]): Dictionary of computed validation metrics,\n                including accuracy, precision, recall, F1-score, MCC, Cohen's kappa,\n                Jaccard index and balanced accuracy (macro-averaged).\n        \"\"\"\n        self.model.train()\n        total_loss = 0.0\n        all_preds, all_targets = [], []\n\n        for metric in self.train_metrics.values():\n            metric.reset()\n\n        for X, y in self.train_loader:\n            X, y = X.to(self.device), y.to(self.device)\n\n            self.optimizer.zero_grad()\n            outputs = self.model(X)\n            loss = self.criterion(outputs, y)\n            loss.backward()\n            self.optimizer.step()\n\n            total_loss += loss.item() * X.size(0)\n\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.append(preds)\n            all_targets.append(y)\n\n        preds_all = torch.cat(all_preds)\n        targets_all = torch.cat(all_targets)\n\n        for metric in self.train_metrics.values():\n            preds = preds_all.to(metric.device)\n            targets = targets_all.to(metric.device)\n            metric.update(preds, targets)\n\n        avg_loss = total_loss / len(self.train_loader.dataset)\n        metrics_out = {f\"train_{k}\": m.compute().item() for k, m in self.train_metrics.items()}\n        return avg_loss, metrics_out\n\n    @torch.inference_mode()\n    def _validate(self) -&gt; tuple[float, list[int], list[int], dict[str, torch.Tensor]]:\n        \"\"\"\n        Runs validation on the current model.\n\n        Returns:\n            avg_loss (float): Average validation loss over the validation dataset.\n            all_preds (list[int]): Flattened list of predicted class indices.\n            all_targets (list[int]): Flattened list of ground truth class indices.\n            metrics_out (dict[str, torch.Tensor]): Dictionary of computed validation metrics,\n                including accuracy, precision, recall, F1-score, MCC, Cohen's kappa,\n                Jaccard index and balanced accuracy (macro-averaged).\n        \"\"\"\n        self.model.eval()\n        total_loss = 0.0\n        all_preds, all_targets = [], []\n\n        for metric in self.val_metrics.values():\n            metric.reset()\n\n        for X, y in self.val_loader:\n            X, y = X.to(self.device), y.to(self.device)\n            outputs = self.model(X)\n            loss = self.criterion(outputs, y)\n\n            preds = torch.argmax(outputs, dim=1)\n\n            total_loss += loss.item() * X.size(0)\n            all_preds.append(preds)\n            all_targets.append(y)\n\n        preds_all = torch.cat(all_preds)\n        targets_all = torch.cat(all_targets)\n\n        for metric in self.val_metrics.values():\n            preds = preds_all.to(metric.device)\n            targets = targets_all.to(metric.device)\n            metric.update(preds, targets)\n\n        avg_loss = total_loss / len(self.val_loader.dataset)\n        metrics_out = {f\"val_{k}\": m.compute().item() for k, m in self.val_metrics.items()}\n\n        return avg_loss, preds_all.cpu().tolist(), targets_all.cpu().tolist(), metrics_out\n</code></pre>"},{"location":"code/training/#src.training.engine.Trainer.train","title":"<code>train()</code>","text":"<p>Main training loop.</p> <p>For each epoch: - Runs training and validation - Logs metrics to wandb (if enabled) - Saves best model (lowest validation loss) - Stores predictions and labels to log confusion matrix later</p> Source code in <code>src/training/engine.py</code> <pre><code>def train(self):\n    \"\"\"\n    Main training loop.\n\n    For each epoch:\n    - Runs training and validation\n    - Logs metrics to wandb (if enabled)\n    - Saves best model (lowest validation loss)\n    - Stores predictions and labels to log confusion matrix later\n    \"\"\"\n    epochs_no_improve = 0\n    for epoch in range(1, self.max_epochs + 1):\n        train_loss, train_metrics = self._train_one_epoch()\n        val_loss, val_preds, val_targets, val_metrics = self._validate()\n\n        if self.scheduler:\n            if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                self.scheduler.step(val_loss)\n            else:\n                self.scheduler.step()\n\n        if self.log_wandb:\n            wandb.log(\n                {\n                    \"epoch\": epoch,\n                    \"train_loss\": train_loss,\n                    \"val_loss\": val_loss,\n                    \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n                    **train_metrics,\n                    **val_metrics,\n                }\n            )\n\n        if val_loss &lt; self.best_val_loss:\n            self.best_val_loss = val_loss\n            torch.save(self.model.state_dict(), self.best_model_path)\n            self._val_preds = val_preds\n            self._val_targets = val_targets\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n\n        if self.early_stopping and epochs_no_improve &gt;= self.early_stopping_patience:\n            print(\n                f\"\u23f9\ufe0f Early stopping at epoch {epoch} (no improvement for {self.early_stopping_patience} epochs)\"\n            )\n            break\n\n    if self.log_wandb and self._val_preds and self._val_targets:\n        if not self.class_names:\n            self.class_names = [str(i) for i in sorted(set(self._val_targets))]\n\n        wandb.log(\n            {\n                \"val_confusion_matrix\": wandb.plot.confusion_matrix(\n                    y_true=self._val_targets,\n                    preds=self._val_preds,\n                    class_names=self.class_names,\n                )\n            }\n        )\n\n    print(f\"\u2705 Best model saved at: {self.best_model_path}\")\n</code></pre>"},{"location":"code/training/#src.training.train.train","title":"<code>train(config=None)</code>","text":"<p>Training entry point compatible with both WandB Sweep and CLI.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary. If None, will be read from CLI.</p> <code>None</code> Source code in <code>src/training/train.py</code> <pre><code>def train(config: dict = None):\n    \"\"\"\n    Training entry point compatible with both WandB Sweep and CLI.\n\n    Args:\n        config (dict, optional): Configuration dictionary. If None, will be read from CLI.\n    \"\"\"\n    if config is None:\n        raise ValueError(\"Configuration dictionary must be provided when using wandb sweep.\")\n    set_seed(42)\n    NUM_WORKERS = os.cpu_count()\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    run = wandb.init(config=config, project=\"Vertebrae Classifier\")\n    config = wandb.config\n\n    train_loader, val_loader = get_dataloders(\n        labels_file_path=LABELS_FILE_PATH,\n        tensor_dir=TENSOR_DIR,\n        train_transforms=None,\n        val_transforms=None,\n        batch_size=config[\"batch_size\"],\n        num_workers=NUM_WORKERS,\n        balance_train=config.get(\"balance_train\", False),\n    )\n    with open(CLASS_NAMES_FILE_PATH) as f:\n        class_names = [line.strip() for line in f if line.strip()]\n\n    model = create_model(\n        model_type=config[\"model_type\"],\n        num_classes=len(class_names),\n        model_depth=config.get(\"model_depth\", 18),\n        shortcut_type=config.get(\"shortcut_type\", \"B\"),\n        freeze_backbone=config.get(\"freeze_backbone\", True),\n        device=DEVICE,\n    )\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n    scheduler = build_scheduler(optimizer=optimizer, config=config)\n\n    trainer = Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        criterion=criterion,\n        device=DEVICE,\n        run_name=run.name,\n        scheduler=scheduler,\n        early_stopping=config[\"early_stopping\"],\n        early_stopping_patience=config[\"early_stopping_patience\"],\n        class_names=class_names,\n        max_epochs=config[\"max_epochs\"],\n        log_wandb=True,\n        save_dir=MODELS_DIR / \"cls\",\n    )\n\n    trainer.train()\n    wandb.finish()\n</code></pre>"}]}